<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>From ProGAN to StyleGAN - Yuthon&#39;s Blog</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="Yusu Pan" /><meta name="description" content="In this post, we are looking into two high-resolution image generation models: ProGAN and StyleGAN. They generates the artificial image gradually, starting from a very low resolution and continuing to a high resolution (finally $1024\times 1024​$).
" /><meta name="keywords" content="yuthon, yusu-pan, blog, deep-learning, computer-vision" />






<meta name="generator" content="Hugo 0.54.0 with even 4.0.0" />


<link rel="canonical" href="https://www.yuthon.com/post/tutorials/from-progan-to-stylegan/" />
<link rel="apple-touch-icon" sizes="180x180" href="https://www.yuthon.com/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://www.yuthon.com/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://www.yuthon.com/favicon-16x16.png">
<link rel="manifest" href="https://www.yuthon.com/manifest.json">
<link rel="mask-icon" href="https://www.yuthon.com/safari-pinned-tab.svg" color="#5bbad5">


<link href="https://www.yuthon.com/dist/even.c2a46f00.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="From ProGAN to StyleGAN" />
<meta property="og:description" content="In this post, we are looking into two high-resolution image generation models: ProGAN and StyleGAN. They generates the artificial image gradually, starting from a very low resolution and continuing to a high resolution (finally $1024\times 1024​$)." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.yuthon.com/post/tutorials/from-progan-to-stylegan/" />
<meta property="article:published_time" content="2018-02-26T19:42:13&#43;08:00"/>
<meta property="article:modified_time" content="2018-12-31T19:42:13&#43;08:00"/>

<meta itemprop="name" content="From ProGAN to StyleGAN">
<meta itemprop="description" content="In this post, we are looking into two high-resolution image generation models: ProGAN and StyleGAN. They generates the artificial image gradually, starting from a very low resolution and continuing to a high resolution (finally $1024\times 1024​$).">


<meta itemprop="datePublished" content="2018-02-26T19:42:13&#43;08:00" />
<meta itemprop="dateModified" content="2018-12-31T19:42:13&#43;08:00" />
<meta itemprop="wordCount" content="6013">



<meta itemprop="keywords" content="GANs,Image Generation," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="From ProGAN to StyleGAN"/>
<meta name="twitter:description" content="In this post, we are looking into two high-resolution image generation models: ProGAN and StyleGAN. They generates the artificial image gradually, starting from a very low resolution and continuing to a high resolution (finally $1024\times 1024​$)."/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="https://www.yuthon.com/" class="logo">Yuthon&#39;s Blog</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="https://www.yuthon.com/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="https://www.yuthon.com/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a><a href="https://www.yuthon.com/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="https://www.yuthon.com/about/">
        <li class="mobile-menu-item">About</li>
      </a><a href="https://www.yuthon.com/resume/">
        <li class="mobile-menu-item">Resume</li>
      </a><a href="https://github.com/corenel">
        <li class="mobile-menu-item">Works</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="https://www.yuthon.com/" class="logo">Yuthon&#39;s Blog</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="https://www.yuthon.com/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="https://www.yuthon.com/categories/">Categories</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="https://www.yuthon.com/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="https://www.yuthon.com/about/">About</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="https://www.yuthon.com/resume/">Resume</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="https://github.com/corenel">Works</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">From ProGAN to StyleGAN</h1>

      <div class="post-meta">
        <span class="post-time"> 2018-02-26 </span>
        <div class="post-category">
            <a href="https://www.yuthon.com/categories/notes/"> Notes </a>
            <a href="https://www.yuthon.com/categories/tutorials/"> Tutorials </a>
            </div>
          <span class="more-meta"> 6013 words </span>
          <span class="more-meta"> 13 mins read </span>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
<ul>
<li>
<ul>
<li><a href="#progan">ProGAN</a>
<ul>
<li><a href="#workflow">Workflow</a></li>
<li><a href="#details">Details</a>
<ul>
<li><a href="#fading-in-new-layers">Fading in new layers</a></li>
<li><a href="#minibatch-standard-deviation">Minibatch Standard Deviation</a></li>
<li><a href="#torgb-fromrgb">toRGB &amp; fromRGB</a></li>
<li><a href="#loss-function">Loss function</a></li>
</ul></li>
<li><a href="#tricks">Tricks</a>
<ul>
<li><a href="#upscale-2x">Upscale 2x</a></li>
<li><a href="#equalized-learning-rate">Equalized Learning Rate</a></li>
<li><a href="#pixel-normalization">Pixel Normalization</a></li>
</ul></li>
<li><a href="#drawbacks">Drawbacks</a></li>
</ul></li>
<li><a href="#stylegan">StyleGAN</a>
<ul>
<li><a href="#workflow-1">Workflow</a></li>
<li><a href="#details-1">Details</a>
<ul>
<li><a href="#mapping-network">Mapping Network</a></li>
<li><a href="#adaptive-instance-normalization-adain">Adaptive Instance Normalization (AdaIN)</a></li>
<li><a href="#removing-traditional-input">Removing traditional input</a></li>
<li><a href="#stochastic-variation">Stochastic variation</a></li>
</ul></li>
<li><a href="#tricks-1">Tricks</a>
<ul>
<li><a href="#style-mixing">Style Mixing</a></li>
</ul></li>
</ul></li>
<li><a href="#reference">Reference</a></li>
</ul></li>
</ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <p>In this post, we are looking into two high-resolution image generation models: ProGAN and StyleGAN. They generates the artificial image gradually, starting from a very low resolution and continuing to a high resolution (finally $1024\times 1024​$).</p>

<table>
<thead>
<tr>
<th>Model</th>
<th>Resources</th>
</tr>
</thead>

<tbody>
<tr>
<td>ProGAN</td>
<td>[<a href="https://arxiv.org/abs/1710.10196">paper</a>] [<a href="https://github.com/tkarras/progressive_growing_of_gans">code (TensorFlow, Official)</a>]</td>
</tr>

<tr>
<td>StyleGAN</td>
<td>[<a href="https://arxiv.org/abs/1812.04948">paper</a>] [<a href="https://github.com/NVlabs/stylegan">code (TensorFlow, Official)</a>]</td>
</tr>
</tbody>
</table>

<h2 id="progan">ProGAN</h2>

<p>NVIDIA在2017年底推出的ProGAN解决了GANs生成高质量高分辨率图像的难题。其核心思想在于渐进式的训练方法（Progressive training）。网络从一个非常低的分辨率（如$4\times 4$）开始，逐步训练并增大图像分辨率，直到生成器能够生成的图像分辨率达到目标的高分辨率（如$1024\times 1024$）。</p>

<figure class="zoomable">
    <img src="https://www.yuthon.com/images/ProGAN_demo.gif"/> <figcaption>
            <h4>ProGAN从低分辨率到高分辨率的渐进式训练示意图 (Source: Sarah Wolf’s blog post on ProGAN)</h4>
        </figcaption>
</figure>


<p>使用渐进式生长训练方式的优势在于：</p>

<ul>
<li>将高分辨率图像生成这样一个复杂的任务，分解为一系列相对简单的任务。这种增量式的学习过程能够很大程度上稳定GANs的训练，并且减少mode collapse现象。</li>
<li>从低分辨率到高分辨率的训练，能够使网络先着眼于在低分辨率图像中也能体现的高层级的图像结构与特征，然后再向其中填充细节。这保证了网络不会在高层级的图像结构中犯大错误，从而提升了生成图像的质量。</li>
<li>此外，渐进生长方式在计算上也比从一开始就训练整个网络的传统方法更为有效。开始时，训练图像分辨率低，网络的层数少，参数量也少，因此网络能够快速收敛。在达到目标分辨率之前，网络都是只有一部分在训练，因此在效率上是有提升的。ProGAN原文中指出，随着目标分辨率的不同，ProGAN的训练速度比传统方法快2至6倍。</li>
</ul>

<figure>
    <img src="https://www.yuthon.com/images/ProGAN_training_time.jpg"/> <figcaption>
            <h4>ProGAN训练时间 (Source: original paper)</h4>
        </figcaption>
</figure>


<h3 id="workflow">Workflow</h3>

<figure>
    <img src="https://www.yuthon.com/images/ProGAN_architecture.jpg"/> <figcaption>
            <h4>ProGAN网络结构 (Source: original paper)</h4>
        </figcaption>
</figure>


<ol>
<li><strong>构建整个ProGAN网络。</strong>ProGAN的网络架构是多尺度的。Generator的每一组都将空间尺寸扩大到原先的两倍，通道数则减少为原先的一半。直到特征的空间尺寸达到目标尺寸，通道数则减少到$3$，及RGB三个通道。Discriminator的网络结构则基本上是Generator的镜像，每一组都减半空间尺寸，倍增通道数。同时，为了保证总参数量不至于过高，倍增得到的通道数的上限设为$512$。</li>
<li><strong>按照分辨率从低到高逐步训练ProGAN网络。</strong>从$4\times 4​$的网络开始训练，稳定后增长分辨率，Generator与Discriminator同时增加一组卷积层，首先进入Fade-in模式，之后进入Stabilize模式。<u>当增加新的卷积层后，原有层内的参数仍然保持可训练的状态。</u>稳定后，继续增大分辨率，如上述进行训练。如此反复，直到达到目标分辨率。</li>
</ol>

<h3 id="details">Details</h3>

<h4 id="fading-in-new-layers">Fading in new layers</h4>

<figure class="zoomable">
    <img src="https://www.yuthon.com/images/ProGAN_generator.png"/> <figcaption>
            <h4>ProGAN网络Generator结构与增长分辨率时的Fade-in策略 (Source: Sarah Wolf’s blog post on ProGAN)</h4>
        </figcaption>
</figure>


<figure class="zoomable">
    <img src="https://www.yuthon.com/images/ProGAN_discriminator.png"/> <figcaption>
            <h4>ProGAN网络Discriminator结构 (Source: [DL輪読会]Progressive Growing of GANs for Improved Quality, Stability, and Variation)</h4>
        </figcaption>
</figure>


<p>当增大分辨率时，Generator与Discriminator都会增加卷积层。为了让新加层快速收敛，同时又不对原有层造成过大的影响，ProGAN提出了一种Fade-in的机制。如上图所示，在Fade-in阶段时，旧有的层的输出经过上采样放大两倍，而后通过<code>toRGB</code>层转化为RGB图像，与新加层的输出通过<code>toRGB</code>层转化后的图像进行加权和，形成最终的输出。这一融合由一个参数$\alpha$进行控制，$x&rsquo;=\alpha x_{i-1} + (1-\alpha) x_i$。随着训练的进行，$\alpha$从$1$线性减少为$0$，最终输出也逐渐转为新加层的输出占主导。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># fade-in phase in generator</span>
<span class="n">upsample</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">progression</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">out</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_rgb</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">out</span><span class="p">)</span>
<span class="n">skip_rgb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_rgb</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">](</span><span class="n">upsample</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">skip_rgb</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">out</span>

<span class="c1"># fade-in phase in discriminator</span>
<span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">progression</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">out</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">avg_pool2d</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">downsample</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">avg_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">skip_rgb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">from_rgb</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">](</span><span class="n">downsample</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">skip_rgb</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">out</span></code></pre></td></tr></table>
</div>
</div>
<h4 id="minibatch-standard-deviation">Minibatch Standard Deviation</h4>

<figure class="zoomable">
    <img src="https://www.yuthon.com/images/ProGAN_minibatch_std.png"/> <figcaption>
            <h4>ProGAN网络Discrimiantor中的minibatch standard deviation层 (Source: Sarah Wolf’s blog post on ProGAN)</h4>
        </figcaption>
</figure>


<p>ProGAN为了解决GANs生成的图像多样性较差的问题，在discriminator的最后增加了一个minibatch standard deviation层。  这个层没有需要训练的参数，其作用为求取minibatch内的所有feature maps ($N\times H\times W \times 3$)上各个像素位置对应的标准差，将这些逐像素的标准差拼起来，组成一张新的feature map ($1\times H\times W \times1 $)作为新的通道加入。这有助于统计minibatch内的信息，让discriminator根据这些额外的统计信息来区分真是样本的batch与生成样本的batch。从而让generator需要生成更加多样化、更加接近真实样本分布的样本来“骗过”discriminator，最终达到增强generator生成多样化样本的目的。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">out_std</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">unbiased</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>
<span class="n">mean_std</span> <span class="o">=</span> <span class="n">out_std</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">mean_std</span> <span class="o">=</span> <span class="n">mean_std</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">out</span><span class="p">,</span> <span class="n">mean_std</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span></code></pre></td></tr></table>
</div>
</div>
<h4 id="torgb-fromrgb">toRGB &amp; fromRGB</h4>

<p>在训练过程中，Generator的输出以及Discriminator的输入需要为RGB图像，这就需要使用$1\times 1$的卷积将在多通道的feature maps与3通道的图像之间进行转换。这就是<code>toRGB</code>（Generator最后一层卷积层）与<code>fromRGB</code>层（Discriminator第一层卷积层）的由来。当然，针对不同分辨率的<code>toRGB</code>层与<code>fromRGB</code>层是不同尺寸的，且是单独训练的。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># toRGB layers in generator</span>
<span class="n">to_rgb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">EqualConv2d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                        <span class="n">EqualConv2d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                        <span class="n">EqualConv2d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                        <span class="n">EqualConv2d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                        <span class="n">EqualConv2d</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                        <span class="n">EqualConv2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)])</span>
<span class="c1"># fromRGB layers in discriminator</span>
<span class="n">from_rgb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">EqualConv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                          <span class="n">EqualConv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                          <span class="n">EqualConv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                          <span class="n">EqualConv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                          <span class="n">EqualConv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                          <span class="n">EqualConv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">1</span><span class="p">)])</span></code></pre></td></tr></table>
</div>
</div>
<h4 id="loss-function">Loss function</h4>

<p>ProGAN使用的是<a href="https://arxiv.org/abs/1704.00028">WGAN-GP</a>，介绍可参见<a href="https://www.yuthon.com/post/tutorials/something-about-gans/#wgan-gp-1">之前的文章</a>。其损失函数形式为：</p>

<div>
$$
\begin{aligned}
\mathcal{L}_G &= -D(x') \\
\mathcal{L}_D &= -D(x) + D(x') + \lambda \times GP \\
GP &= \left(||\nabla D (\alpha x' + (1 - \alpha)x ))||_2 - 1 \right)^2 \\
\end{aligned}
$$
</div>

<p>其中，$D$为Discriminator，$x$、$x&rsquo;$分别为真实样本与生成样本。$\lambda=10$为权重项，$GP$为用于稳定训练的梯度惩罚项，$\alpha \in (0,1)$为均匀采样的随机数，用以表示$x$与$x&rsquo;$的加权平均（即其连线上的任意一点）。</p>

<h3 id="tricks">Tricks</h3>

<h4 id="upscale-2x">Upscale 2x</h4>

<p>在放大特征图的方法上，与DCGAN等使用转置卷积（transpose convolutions）不同，ProGAN用最近邻插值来视线上采样，用average pooling来降采样。这两种方法均不需要可学习的参数，更为简单。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="c1"># upsample = nn.F.interpolate(out, scale_factor=2, mode=&#39;nearest&#39;, align_corners=False)</span>
<span class="n">upsampler</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">UpsamplingNearest2d</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># downsample = F.avg_pool2d(out, 2)</span>
<span class="n">downsampler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span></code></pre></td></tr></table>
</div>
</div>
<h4 id="equalized-learning-rate">Equalized Learning Rate</h4>

<p>为了保证Generator与Discriminator之间的良性竞争，ProGAN指出需要使得各个卷积层以相似的速度进行学习。为了达到equlized learnig rate，ProGAN采用了与<a href="https://arxiv.org/abd/1502.01852">He initialization</a>相似的方法，也就是将每个层的权重乘以其权重参数量。而且不仅仅是初始化权重时这么做，在训练过程的每次forwarding时都进行此操作。</p>

<div>
$$
W = W_{orig} \times \sqrt{\frac{2}{\text{fan_in}}}
$$
</div>

<p>其中$W_{orig}$、$W$分别为原始权重与实际使用的权重，对于卷积层来说，$\text{fan_in} = k\times k \times c$$k$为kernel大小，$c$为</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">EqualLR</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>

    <span class="k">def</span> <span class="nf">compute_weight</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">):</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;_orig&#39;</span><span class="p">)</span>
        <span class="n">fan_in</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">weight</span> <span class="o">*</span> <span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="n">fan_in</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">apply</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="n">fn</span> <span class="o">=</span> <span class="n">EqualLR</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>

        <span class="n">weight</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
        <span class="k">del</span> <span class="n">module</span><span class="o">.</span><span class="n">_parameters</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
        <span class="n">module</span><span class="o">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;_orig&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">))</span>
        <span class="n">module</span><span class="o">.</span><span class="n">register_forward_pre_hook</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">fn</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">):</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_weight</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
        <span class="nb">setattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">equal_lr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;weight&#39;</span><span class="p">):</span>
    <span class="n">EqualLR</span><span class="o">.</span><span class="nb">apply</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<span class="k">class</span> <span class="nc">EqualConv2d</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">conv</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">normal_</span><span class="p">()</span>
        <span class="n">conv</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">equal_lr</span><span class="p">(</span><span class="n">conv</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span></code></pre></td></tr></table>
</div>
</div>
<h4 id="pixel-normalization">Pixel Normalization</h4>

<p>ProGAN没有使用BN层，而是提出了Pixel Normalization层。与BN层类似，PN层直接放在卷积层之后，激活函数之前。这个层没有需要训练的参数，其作用为将feature maps中的每个像素位置$(x,y)$在不同的通道$C​$上的值都归一化到单位长度：</p>

<div>
$$
b_{x,y} = \frac{a_{x,y}}{\sqrt{\frac{1}{C} \sum^C_{j=0} a_{x,y}^j+\epsilon}}
$$
</div>

<p>其中，$a$、$b$分别为输入张量与输出张量，$\epsilon=10^{-8}$为防止除以零的常数。这一举措能够防止像素位置上的信号响应在训练过程中失控，可以提升训练时的稳定性。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">PixelNorm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_tensor</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">input_tensor</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span></code></pre></td></tr></table>
</div>
</div>
<h3 id="drawbacks">Drawbacks</h3>

<p>ProGAN虽然能够生成高质量高分辨率的图像，但是其本质上还是一种无条件（unconditional）的生成方法。其难以控制所生成图像的属性。并且就算是修改输入的随机向量，其微小的变化也会引起最终生成图像中的多个属性一起变化。如何将ProGAN改为有条件（conditional）的生成模型，或者增强其微调单个属性的能力，这是一个可以研究的方向。</p>

<h2 id="stylegan">StyleGAN</h2>

<p>StyleGAN是NVIDIA继ProGAN之后提出的新的生成网络，其主要通过分别修改每一层级的输入，在不影响其他层级的情况下，来控制该层级所表示的视觉特征。这些特征可以是粗的特征（如姿势、脸型等），也可以是一些细节特征（如瞳色、发色等）。</p>

<figure class="zoomable">
    <img src="https://www.yuthon.com/images/StyleGAN_visualization.jpg"/> <figcaption>
            <h4>StyleGAN可视化结果 (Source: original paper)</h4>
        </figcaption>
</figure>


<p>具体地说，StyleGAN提出，如果训练得当，ProGAN的每一个层级都有能力控制不同的视觉特征。层级越低，分辨率越低，其能控制的视觉特征也就越粗糙。因此，StyleGAN将视觉特征划分为三类：</p>

<ol>
<li>粗糙（初级）特征：分辨率小于$8\times 8$，主要影响姿态、大致发型、脸型等；</li>
<li>中级特征：分辨率介于$16\times 16$至$32\times 32$之间，主要影响更加细节的脸部特征、细节发型、嘴的张闭等；</li>
<li>细节（高级）特征：分辨率介于$64\times 64$至$1024\times 1024​$之间，主要影响整体的色调（发色、肤色以及背景色等）与一些细微的特征。</li>
</ol>

<h3 id="workflow-1">Workflow</h3>

<figure class="zoomable">
    <img src="https://www.yuthon.com/images/StyleGAN_overview.jpg"/> <figcaption>
            <h4>StyleGAN网络结构 (Source: original paper)</h4>
        </figcaption>
</figure>


<ol>
<li>从先验分布$\mathcal{Z}$中采样一个一个$512\times 1$的随机向量$\mathbf{z} \in \mathcal{Z}$作为latent code，归一化后经过Mapping Network映射到另外一个中间的latent space上，得到中间的latent code表示$\mathbf{w} \in \mathcal{W}$</li>
<li>将上一步得到的$\mathbf{w}$通过可学习的仿射变换$A$输入到Synthesis Network各个层级的AdaIN层中中，用以控制style；同时将噪声通过学习到的缩放参数$B$加到AdaIN层之前</li>
<li>将固定的向量输入Synthesis Network，输出得到生成的图像。</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">StyledConvBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channel</span><span class="p">,</span> <span class="n">out_channel</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                 <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">style_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">initial</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">initial</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">ConstantInput</span><span class="p">(</span><span class="n">in_channel</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">EqualConv2d</span><span class="p">(</span><span class="n">in_channel</span><span class="p">,</span> <span class="n">out_channel</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">noise1</span> <span class="o">=</span> <span class="n">equal_lr</span><span class="p">(</span><span class="n">NoiseInjection</span><span class="p">(</span><span class="n">out_channel</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">adain1</span> <span class="o">=</span> <span class="n">AdaptiveInstanceNorm</span><span class="p">(</span><span class="n">out_channel</span><span class="p">,</span> <span class="n">style_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lrelu1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">EqualConv2d</span><span class="p">(</span><span class="n">out_channel</span><span class="p">,</span> <span class="n">out_channel</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">noise2</span> <span class="o">=</span> <span class="n">equal_lr</span><span class="p">(</span><span class="n">NoiseInjection</span><span class="p">(</span><span class="n">out_channel</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">adain2</span> <span class="o">=</span> <span class="n">AdaptiveInstanceNorm</span><span class="p">(</span><span class="n">out_channel</span><span class="p">,</span> <span class="n">style_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lrelu2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">style</span><span class="p">,</span> <span class="n">noise</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise1</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">noise</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adain1</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">style</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lrelu1</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise2</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">noise</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adain2</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">style</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lrelu2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span>
    
    
<span class="k">class</span> <span class="nc">Generator</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">code_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">progression</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">StyledConvBlock</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">initial</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
                                          <span class="n">StyledConvBlock</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                                          <span class="n">StyledConvBlock</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                                          <span class="n">StyledConvBlock</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                                          <span class="n">StyledConvBlock</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                                          <span class="n">StyledConvBlock</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">to_rgb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">EqualConv2d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                                     <span class="n">EqualConv2d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                                     <span class="n">EqualConv2d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                                     <span class="n">EqualConv2d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                                     <span class="n">EqualConv2d</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                                     <span class="n">EqualConv2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)])</span>

        <span class="c1"># self.blur = Blur()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">style</span><span class="p">,</span> <span class="n">noise</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">mixing_range</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)):</span>
        <span class="k">pass</span></code></pre></td></tr></table>
</div>
</div>
<h3 id="details-1">Details</h3>

<h4 id="mapping-network">Mapping Network</h4>

<p>Mapping Network的作用是将输入向量编码为一个中间表示，使得该中间表示的每一个元素都能够控制不同的视觉特征。</p>

<p>如果像传统的cGANs及其衍生版本那样，只靠输入向量自身控制视觉特征，这种能力是有限的，因为其还要受到训练数据的概率密度分布的影响。训练数据中如果某一类出现得多一些，那么输入向量中的值就更可能被映射到这一类上面。这就导致了模型所控制的特征是耦合的（coupled）或者说是纠缠的（entangled），模型并不能单独控制输入向量的某一部分的映射。但是通过Mapping Network将输入向量映射为另外的中间表示，则不用服从训练数据集的分布，并且能够在一定程度上减少特征之间的相关性。</p>

<p>Mapping Network由8层FC层组成，输入为随机向量$\mathbf{z} \in \mathcal{Z}$，输出为中间表示$\mathbf{w} \in \mathcal{W}$，两者维度均为$512\times 1$。</p>

<figure class="zoomable">
    <img src="https://www.yuthon.com/images/StyleGAN_mapping_network.png"/> <figcaption>
            <h4>StyleGAN网络中的Mapping Network (Source: Rani Horev&#39;s blog post)</h4>
        </figcaption>
</figure>

<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">StyledGenerator</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">code_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">n_mlp</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">generator</span> <span class="o">=</span> <span class="n">Generator</span><span class="p">(</span><span class="n">code_dim</span><span class="p">)</span>

        <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">PixelNorm</span><span class="p">()]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_mlp</span><span class="p">):</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">EqualLinear</span><span class="p">(</span><span class="n">code_dim</span><span class="p">,</span> <span class="n">code_dim</span><span class="p">))</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">style</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">mean_style</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">style_weight</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">mixing_range</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)):</span>
		<span class="k">pass</span></code></pre></td></tr></table>
</div>
</div>
<h4 id="adaptive-instance-normalization-adain">Adaptive Instance Normalization (AdaIN)</h4>

<p>Mapping Network编码得到的中间表示$\mathbf{w} \in \mathcal{W}$，需要通过AdaIN (Adaptive Instance Normalization)来输入生成网络。AdaIN层在Systhesis Network的每个分辨率层级中都存在，并且用以控制该分辨率层级的视觉特征。</p>

<div>
$$
\text{AdaIN}(x_i,y) = y_{s,i} \frac{x_i - \mu(x_i)}{\sigma(x_i)} + y_{b,i}
$$
</div>

<ol>
<li>对卷积层的输出进行Instance Normalization，也就是将输出的每个通道都进行归一化，得到<code>$\frac{x_i - \mu(x_i)}{\sigma(x_i)}$</code></li>
<li>对输入的中间表示$\mathbf{w}$（维度$512$）通过一个FC层$A$转换为针对$n$个通道的scale (<code>$y_{s,i}$</code>)与bias (<code>$y_{b,i}$</code>)，维度为$2n$</li>
<li>通过第2部得到的scale与bias，对于第1步得到的归一化输出的每个通道都进行shift。这种操作相当于对卷积层的每个滤波器的输出进行加权平均，而这个权重是可学习的。通过训练，使得$\mathbf{w}$所代表的权重能够被转化为视觉表示。</li>
</ol>

<figure class="zoomable">
    <img src="https://www.yuthon.com/images/StyleGAN_adain.png"/> <figcaption>
            <h4>StyleGAN网络中的AdaIN模块 (Source: Rani Horev&#39;s blog post)</h4>
        </figcaption>
</figure>

<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">AdaptiveInstanceNorm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channel</span><span class="p">,</span> <span class="n">style_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm2d</span><span class="p">(</span><span class="n">in_channel</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">style</span> <span class="o">=</span> <span class="n">EqualLinear</span><span class="p">(</span><span class="n">style_dim</span><span class="p">,</span> <span class="n">in_channel</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="p">[:</span><span class="n">in_channel</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">in_channel</span><span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">style</span><span class="p">):</span>
        <span class="c1"># 512 -&gt; Nx2</span>
        <span class="n">style</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">style</span><span class="p">(</span><span class="n">style</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
        <span class="c1"># split sytle inyo scale and bias</span>
        <span class="n">gamma</span><span class="p">,</span> <span class="n">beta</span> <span class="o">=</span> <span class="n">style</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">out</span> <span class="o">+</span> <span class="n">beta</span>

        <span class="k">return</span> <span class="n">out</span></code></pre></td></tr></table>
</div>
</div>
<h4 id="removing-traditional-input">Removing traditional input</h4>

<p>包含之前的ProGAN在内，传统的GANs都需要用一个随机向量喂给生成网络来生成图像，这个随机向量就决定了生成图像的视觉特征。而在StyleGAN中，既然生成图像的视觉特征已经交由$\mathbf{w}$与AdaIn来控制，那么再在Synthesis Network的最开始输入一个随机向量就显得有点多余了。因此这个随机向量输入被替换成了一个定值向量输入，而且这在结果上有益于生成图像的质量。一个可能的解释是这种固定的输入使得网络只需要考虑$\mathbf{w}$那边传过来的视觉属性，而不用再管另外一个输入的变量，从而在一定程度上减少特征之间的纠缠。</p>

<figure class="zoomable">
    <img src="https://www.yuthon.com/images/StyleGAN_constant_input.png"/> <figcaption>
            <h4>StyleGAN网络在Synthesis Network上使用了固定的输入 (Source: Rani Horev&#39;s blog post)</h4>
        </figcaption>
</figure>

<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">ConstantInput</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
		
        <span class="c1"># generate fixed random vector</span>
        <span class="bp">self</span><span class="o">.</span><span class="nb">input</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="c1"># generate fake batch</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="nb">input</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span></code></pre></td></tr></table>
</div>
</div>
<h4 id="stochastic-variation">Stochastic variation</h4>

<p>为了增强生成样本的多样性，同时考虑到人脸上还是有许多地方可以看成是随机的（例如雀斑、皱纹、头发纹理等等），通常GANs会在输入向量上增加一层随机噪声来实现这种微小的特征。StyleGAN也一样，如果只使用$\mathbf{w}$来控制视觉特征，输入Synthesis Network的向量又是固定的，那么一旦$\mathbf{w}$固定，则生成的图像也是一成不变的。</p>

<p>不过，如上文所述，传统方法直接将随机噪声加在输入变量上，这样容易导致特征的纠缠现象，使得其他的特征也受到影响。同样地，与上面的解决方法一致，StyleGAN将噪声通过FC层$B$重新编码，然后加在AdaIN之前一层输出的每个通道上，用以轻微改变每一层级所负责的视觉特征。</p>

<figure class="zoomable">
    <img src="https://www.yuthon.com/images/StyleGAN_noise.png"/> <figcaption>
            <h4>StyleGAN网络在AdaIN层之前增加了编码后的噪声 (Source: Rani Horev&#39;s blog post)</h4>
        </figcaption>
</figure>

<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">NoiseInjection</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">channel</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="n">noise</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">image</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">*</span> <span class="n">noise</span></code></pre></td></tr></table>
</div>
</div>
<h3 id="tricks-1">Tricks</h3>

<h4 id="style-mixing">Style Mixing</h4>

<p>(to be continued)</p>

<h2 id="reference">Reference</h2>

<ul>
<li><a href="https://towardsdatascience.com/progan-how-nvidia-generated-images-of-unprecedented-quality-51c98ec2cbd2">ProGAN: How NVIDIA Generated Images of Unprecedented Quality</a></li>
<li><a href="https://towardsdatascience.com/progressively-growing-gans-9cb795caebee">Progressively-Growing GANs Review</a></li>
<li><a href="https://towardsdatascience.com/explained-a-style-based-generator-architecture-for-gans-generating-and-tuning-realistic-6cb2be0f431">Explained: A Style-Based Generator Architecture for GANs - Generating and Tuning Realistic Artificial Faces</a></li>
<li><a href="rosinality/style-based-gan-pytorch">rosinality/style-based-gan-pytorch</a></li>
</ul>
    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">Author</span>
    <span class="item-content">Yusu Pan</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">LastMod</span>
    <span class="item-content">
        2018-12-31
        
    </span>
  </p>
  
  <p class="copyright-item">
    <span class="item-title">License</span>
    <span class="item-content"><a rel="license noopener" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank">CC BY-NC-ND 4.0</a></span>
  </p>
</div>
<footer class="post-footer">
      <div class="post-tags">
          <a href="https://www.yuthon.com/tags/gans/">GANs</a>
          <a href="https://www.yuthon.com/tags/image-generation/">Image Generation</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="https://www.yuthon.com/post/tutorials/notes-for-object-detection-one-stage-methods/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">Notes for Object Detection: One Stage Methods</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        <a class="next" href="https://www.yuthon.com/post/papers/notes-for-adversarial-discriminative-domain-adaptation/">
            <span class="next-text nav-default">Notes for Adversarial Discriminative Domain Adaptation</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        <div id="disqus_thread"></div>
    <script type="text/javascript">
    (function() {
      
      
      if (window.location.hostname === 'localhost') return;

      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      var disqus_shortname = 'yuthons-blog';
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:xxdsox@gmail.com" class="iconfont icon-email" title="email"></a>
      <a href="https://stackoverflow.com/users/5682817" class="iconfont icon-stack-overflow" title="stack-overflow"></a>
      <a href="https://twitter.com/corenel" class="iconfont icon-twitter" title="twitter"></a>
      <a href="https://www.facebook.com/xxdsox" class="iconfont icon-facebook" title="facebook"></a>
      <a href="https://github.com/corenel" class="iconfont icon-github" title="github"></a>
      <a href="https://www.zhihu.com/people/pan-yu-su" class="iconfont icon-zhihu" title="zhihu"></a>
  <a href="https://www.yuthon.com/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2017 - 
    2019
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">© This post is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License, please give source if you wish to quote or reproduce.
</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>
<script type="text/javascript" src="https://www.yuthon.com/dist/even.26188efa.min.js"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      TeX: {equationNumbers: {autoNumber: "AMS"}},
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script type="text/javascript" async src="https://www.yuthon.com/lib/mathjax/math-code.js"></script>
    <script async src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"  integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script>


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-76233259-1', 'auto');
	ga('set', 'anonymizeIp', true);
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>







</body>
</html>
