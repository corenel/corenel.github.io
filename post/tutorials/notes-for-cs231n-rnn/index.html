<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>Notes for CS231n Recurrent Neural Network - Yuthon&#39;s Blog</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="Yusu Pan" /><meta name="description" content="从 RNN 开始, CS231n 的 Lecture Notes 就没有了, 因此我根据上课时的 Slides 整理了一些需要重视的知识点. 还可以参考这些文章或是视频来加深理解。 Lecture 10 Introduction Recurrent Networks offer a lot of flexibility: one to one: Vanilla Neural Networks one to many: e.g. Image Captioning (image -&amp;gt; sequence of words) many to one: e.g. Sentiment Classification (sequence of words -&amp;gt; sentiment) many to many: e.g. Machine" /><meta name="keywords" content="yuthon, yusu-pan, blog, deep-learning, computer-vision" />






<meta name="generator" content="Hugo 0.54.0 with even 4.0.0" />


<link rel="canonical" href="https://www.yuthon.com/post/tutorials/notes-for-cs231n-rnn/" />
<link rel="apple-touch-icon" sizes="180x180" href="https://www.yuthon.com/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://www.yuthon.com/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://www.yuthon.com/favicon-16x16.png">
<link rel="manifest" href="https://www.yuthon.com/manifest.json">
<link rel="mask-icon" href="https://www.yuthon.com/safari-pinned-tab.svg" color="#5bbad5">


<link href="https://www.yuthon.com/dist/even.c2a46f00.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="Notes for CS231n Recurrent Neural Network" />
<meta property="og:description" content="从 RNN 开始, CS231n 的 Lecture Notes 就没有了, 因此我根据上课时的 Slides 整理了一些需要重视的知识点. 还可以参考这些文章或是视频来加深理解。 Lecture 10 Introduction Recurrent Networks offer a lot of flexibility: one to one: Vanilla Neural Networks one to many: e.g. Image Captioning (image -&gt; sequence of words) many to one: e.g. Sentiment Classification (sequence of words -&gt; sentiment) many to many: e.g. Machine" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.yuthon.com/post/tutorials/notes-for-cs231n-rnn/" />
<meta property="article:published_time" content="2016-10-30T14:59:17&#43;00:00"/>
<meta property="article:modified_time" content="2016-10-30T14:59:17&#43;00:00"/>

<meta itemprop="name" content="Notes for CS231n Recurrent Neural Network">
<meta itemprop="description" content="从 RNN 开始, CS231n 的 Lecture Notes 就没有了, 因此我根据上课时的 Slides 整理了一些需要重视的知识点. 还可以参考这些文章或是视频来加深理解。 Lecture 10 Introduction Recurrent Networks offer a lot of flexibility: one to one: Vanilla Neural Networks one to many: e.g. Image Captioning (image -&gt; sequence of words) many to one: e.g. Sentiment Classification (sequence of words -&gt; sentiment) many to many: e.g. Machine">


<meta itemprop="datePublished" content="2016-10-30T14:59:17&#43;00:00" />
<meta itemprop="dateModified" content="2016-10-30T14:59:17&#43;00:00" />
<meta itemprop="wordCount" content="1688">



<meta itemprop="keywords" content="CS231n,Recurrent Neural Network,Deep Learning," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Notes for CS231n Recurrent Neural Network"/>
<meta name="twitter:description" content="从 RNN 开始, CS231n 的 Lecture Notes 就没有了, 因此我根据上课时的 Slides 整理了一些需要重视的知识点. 还可以参考这些文章或是视频来加深理解。 Lecture 10 Introduction Recurrent Networks offer a lot of flexibility: one to one: Vanilla Neural Networks one to many: e.g. Image Captioning (image -&gt; sequence of words) many to one: e.g. Sentiment Classification (sequence of words -&gt; sentiment) many to many: e.g. Machine"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="https://www.yuthon.com/" class="logo">Yuthon&#39;s Blog</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="https://www.yuthon.com/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="https://www.yuthon.com/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a><a href="https://www.yuthon.com/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="https://www.yuthon.com/about/">
        <li class="mobile-menu-item">About</li>
      </a><a href="https://www.yuthon.com/resume/">
        <li class="mobile-menu-item">Resume</li>
      </a><a href="https://github.com/corenel">
        <li class="mobile-menu-item">Works</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="https://www.yuthon.com/" class="logo">Yuthon&#39;s Blog</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="https://www.yuthon.com/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="https://www.yuthon.com/categories/">Categories</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="https://www.yuthon.com/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="https://www.yuthon.com/about/">About</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="https://www.yuthon.com/resume/">Resume</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="https://github.com/corenel">Works</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">Notes for CS231n Recurrent Neural Network</h1>

      <div class="post-meta">
        <span class="post-time"> 2016-10-30 </span>
        <div class="post-category">
            <a href="https://www.yuthon.com/categories/tutorials/"> Tutorials </a>
            </div>
          <span class="more-meta"> 1688 words </span>
          <span class="more-meta"> 4 mins read </span>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
<ul>
<li><a href="#lecture-10">Lecture 10</a>
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#recurrent-neural-network">Recurrent Neural Network</a>
<ul>
<li><a href="#concept">Concept</a></li>
<li><a href="#example-character-level-language-model">Example: Character-level language model</a></li>
<li><a href="#example-image-captioning">Example: Image Captioning</a></li>
<li><a href="#more-examples">More examples</a></li>
</ul></li>
<li><a href="#long-short-term-memory-lstm">Long Short Term Memory (LSTM)</a>
<ul>
<li><a href="#vanishing-exploding-gradients">Vanishing/Exploding gradients</a></li>
<li><a href="#introduction-1">Introduction</a></li>
<li><a href="#concept-1">Concept</a></li>
</ul></li>
<li><a href="#summary">Summary</a></li>
</ul></li>
<li><a href="#references">References</a></li>
</ul>
</nav>
  </div>
</div>
    <div class="post-content">
      

<p>从 RNN 开始, CS231n 的 Lecture Notes 就没有了, 因此我根据上课时的 Slides 整理了一些需要重视的知识点. 还可以参考这些文章或是视频来加深理解。</p>

<!-- more -->

<h1 id="lecture-10">Lecture 10</h1>

<h2 id="introduction">Introduction</h2>

<p>Recurrent Networks offer a lot of flexibility:</p>

<p><img src="https://www.yuthon.com/images/RNN_flexibility.png" alt="RNN_flexibility" /></p>

<ol>
<li><strong>one to one</strong>: Vanilla Neural Networks</li>
<li><strong>one to many</strong>: e.g. Image Captioning (image -&gt; sequence of words)</li>
<li><strong>many to one</strong>: e.g. Sentiment Classification (sequence of words -&gt; sentiment)</li>
<li><strong>many to many</strong>:

<ul>
<li>e.g. Machine Translation (seq of words -&gt; seq of words)</li>
<li>e.g. Video classification on frame level</li>
</ul></li>
</ol>

<p>RNN can also do sequential precessing of fix inputs (Multiple Object Recognition with Visual Attention, Ba et al.) or fixed outputs (DRAW: A Recurrent Neural Network For Image Generation, Gregor et al.).</p>

<h2 id="recurrent-neural-network">Recurrent Neural Network</h2>

<h3 id="concept">Concept</h3>

<p>Usually we want to predict a vector at some time steps. To achieve this goal, we can process a sequence of vectors $x$ by applying a recurrence formula at every time step:</p>

<p><img src="https://www.yuthon.com/images/RNN_concept.png" alt="RNN_concept" /></p>

<blockquote>
<p>Notice: the same function and the same set of parameters are used at every time step. That&rsquo;s to say, we use <strong>shared weights</strong>.</p>
</blockquote>

<p><strong>(Vanilla) Recurrent Neural Network</strong></p>

<p>The state consists of a single &ldquo;hidden&rdquo; vector $h$:</p>

<ul>
<li>$h_t = tanh (W_{hh} h_{t-1} + W_{xh} x_t)$</li>
<li>$y_t = W_{hy} h_t$</li>
</ul>

<h3 id="example-character-level-language-model">Example: Character-level language model</h3>

<p>We have a vocabulary of four characters $\begin{bmatrix} h &amp; e &amp; l &amp; o \end{bmatrix}$, and the example training sequence is &ldquo;hello&rdquo;.</p>

<p><img src="https://www.yuthon.com/images/RNN_character-level_language_model_example.png" alt="character-level_language_model_example" /></p>

<p>And we can look its <a href="https://gist.github.com/karpathy/d4dee566867f8291f086">the implement</a>.</p>

<p><strong>Data I/O</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="s2">&#34;&#34;&#34;
</span><span class="s2">Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)
</span><span class="s2">BSD License
</span><span class="s2">&#34;&#34;&#34;</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="c1"># data I/O</span>
<span class="n">data</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;input.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span> <span class="c1"># should be simple plain text file</span>
<span class="n">chars</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
<span class="n">data_size</span><span class="p">,</span> <span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>
<span class="k">print</span> <span class="s1">&#39;data has </span><span class="si">%d</span><span class="s1"> characters, </span><span class="si">%d</span><span class="s1"> unique.&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">data_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
<span class="n">char_to_ix</span> <span class="o">=</span> <span class="p">{</span> <span class="n">ch</span><span class="p">:</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">ch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span> <span class="p">}</span>
<span class="n">ix_to_char</span> <span class="o">=</span> <span class="p">{</span> <span class="n">i</span><span class="p">:</span><span class="n">ch</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">ch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span> <span class="p">}</span></code></pre></td></tr></table>
</div>
</div>
<p><strong>Initializations</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># hyperparameters</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">100</span> <span class="c1"># size of hidden layer of neurons</span>
<span class="n">seq_length</span> <span class="o">=</span> <span class="mi">25</span> <span class="c1"># number of steps to unroll the RNN for</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-1</span>

<span class="c1"># model parameters</span>
<span class="n">Wxh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span><span class="o">*</span><span class="mf">0.01</span> <span class="c1"># input to hidden</span>
<span class="n">Whh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span><span class="o">*</span><span class="mf">0.01</span> <span class="c1"># hidden to hidden</span>
<span class="n">Why</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span><span class="o">*</span><span class="mf">0.01</span> <span class="c1"># hidden to output</span>
<span class="n">bh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="c1"># hidden bias</span>
<span class="n">by</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">vocab_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="c1"># output bias</span></code></pre></td></tr></table>
</div>
</div>
<p><strong>Main Loop</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
<span class="n">mWxh</span><span class="p">,</span> <span class="n">mWhh</span><span class="p">,</span> <span class="n">mWhy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Wxh</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Whh</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Why</span><span class="p">)</span>
<span class="n">mbh</span><span class="p">,</span> <span class="n">mby</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">bh</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">by</span><span class="p">)</span> <span class="c1"># memory variables for Adagrad</span>
<span class="n">smooth_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">1.0</span><span class="o">/</span><span class="n">vocab_size</span><span class="p">)</span><span class="o">*</span><span class="n">seq_length</span> <span class="c1"># loss at iteration 0</span>
<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
  <span class="c1"># prepare inputs (we&#39;re sweeping from left to right in steps seq_length long)</span>
  <span class="k">if</span> <span class="n">p</span><span class="o">+</span><span class="n">seq_length</span><span class="o">+</span><span class="mi">1</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="ow">or</span> <span class="n">n</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">hprev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">hidden_size</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># reset RNN memory</span>
    <span class="n">p</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># go from start of data</span>
  <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">char_to_ix</span><span class="p">[</span><span class="n">ch</span><span class="p">]</span> <span class="k">for</span> <span class="n">ch</span> <span class="ow">in</span> <span class="n">data</span><span class="p">[</span><span class="n">p</span><span class="p">:</span><span class="n">p</span><span class="o">+</span><span class="n">seq_length</span><span class="p">]]</span>
  <span class="n">targets</span> <span class="o">=</span> <span class="p">[</span><span class="n">char_to_ix</span><span class="p">[</span><span class="n">ch</span><span class="p">]</span> <span class="k">for</span> <span class="n">ch</span> <span class="ow">in</span> <span class="n">data</span><span class="p">[</span><span class="n">p</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span><span class="n">p</span><span class="o">+</span><span class="n">seq_length</span><span class="o">+</span><span class="mi">1</span><span class="p">]]</span>

  <span class="c1"># sample from the model now and then</span>
  <span class="k">if</span> <span class="n">n</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">sample_ix</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">hprev</span><span class="p">,</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">200</span><span class="p">)</span>
    <span class="n">txt</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">ix_to_char</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="k">for</span> <span class="n">ix</span> <span class="ow">in</span> <span class="n">sample_ix</span><span class="p">)</span>
    <span class="k">print</span> <span class="s1">&#39;----</span><span class="se">\n</span><span class="s1"> </span><span class="si">%s</span><span class="s1"> </span><span class="se">\n</span><span class="s1">----&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">txt</span><span class="p">,</span> <span class="p">)</span>

  <span class="c1"># forward seq_length characters through the net and fetch gradient</span>
  <span class="n">loss</span><span class="p">,</span> <span class="n">dWxh</span><span class="p">,</span> <span class="n">dWhh</span><span class="p">,</span> <span class="n">dWhy</span><span class="p">,</span> <span class="n">dbh</span><span class="p">,</span> <span class="n">dby</span><span class="p">,</span> <span class="n">hprev</span> <span class="o">=</span> <span class="n">lossFun</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">hprev</span><span class="p">)</span>
  <span class="n">smooth_loss</span> <span class="o">=</span> <span class="n">smooth_loss</span> <span class="o">*</span> <span class="mf">0.999</span> <span class="o">+</span> <span class="n">loss</span> <span class="o">*</span> <span class="mf">0.001</span>
  <span class="k">if</span> <span class="n">n</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="k">print</span> <span class="s1">&#39;iter </span><span class="si">%d</span><span class="s1">, loss: </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">smooth_loss</span><span class="p">)</span> <span class="c1"># print progress</span>

  <span class="c1"># perform parameter update with Adagrad</span>
  <span class="k">for</span> <span class="n">param</span><span class="p">,</span> <span class="n">dparam</span><span class="p">,</span> <span class="n">mem</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="n">Wxh</span><span class="p">,</span> <span class="n">Whh</span><span class="p">,</span> <span class="n">Why</span><span class="p">,</span> <span class="n">bh</span><span class="p">,</span> <span class="n">by</span><span class="p">],</span>
                                <span class="p">[</span><span class="n">dWxh</span><span class="p">,</span> <span class="n">dWhh</span><span class="p">,</span> <span class="n">dWhy</span><span class="p">,</span> <span class="n">dbh</span><span class="p">,</span> <span class="n">dby</span><span class="p">],</span>
                                <span class="p">[</span><span class="n">mWxh</span><span class="p">,</span> <span class="n">mWhh</span><span class="p">,</span> <span class="n">mWhy</span><span class="p">,</span> <span class="n">mbh</span><span class="p">,</span> <span class="n">mby</span><span class="p">]):</span>
    <span class="n">mem</span> <span class="o">+=</span> <span class="n">dparam</span> <span class="o">*</span> <span class="n">dparam</span>
    <span class="n">param</span> <span class="o">+=</span> <span class="o">-</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dparam</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mem</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span> <span class="c1"># adagrad update</span>

  <span class="n">p</span> <span class="o">+=</span> <span class="n">seq_length</span> <span class="c1"># move data pointer</span>
  <span class="n">n</span> <span class="o">+=</span> <span class="mi">1</span> <span class="c1"># iteration counter</span></code></pre></td></tr></table>
</div>
</div>
<p><strong>Loss function</strong></p>

<ul>
<li>forward pass (compute loss)</li>
<li>backward pass (compute param gradient)</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">lossFun</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">hprev</span><span class="p">):</span>
  <span class="s2">&#34;&#34;&#34;
</span><span class="s2">  inputs,targets are both list of integers.
</span><span class="s2">  hprev is Hx1 array of initial hidden state
</span><span class="s2">  returns the loss, gradients on model parameters, and last hidden state
</span><span class="s2">  &#34;&#34;&#34;</span>
  <span class="n">xs</span><span class="p">,</span> <span class="n">hs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">ps</span> <span class="o">=</span> <span class="p">{},</span> <span class="p">{},</span> <span class="p">{},</span> <span class="p">{}</span>
  <span class="n">hs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">hprev</span><span class="p">)</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="c1"># forward pass</span>
  <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)):</span>
    <span class="n">xs</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">vocab_size</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># encode in 1-of-k representation</span>
    <span class="n">xs</span><span class="p">[</span><span class="n">t</span><span class="p">][</span><span class="n">inputs</span><span class="p">[</span><span class="n">t</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">hs</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Wxh</span><span class="p">,</span> <span class="n">xs</span><span class="p">[</span><span class="n">t</span><span class="p">])</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Whh</span><span class="p">,</span> <span class="n">hs</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="n">bh</span><span class="p">)</span> <span class="c1"># hidden state</span>
    <span class="n">ys</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Why</span><span class="p">,</span> <span class="n">hs</span><span class="p">[</span><span class="n">t</span><span class="p">])</span> <span class="o">+</span> <span class="n">by</span> <span class="c1"># unnormalized log probabilities for next chars</span>
    <span class="n">ps</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">ys</span><span class="p">[</span><span class="n">t</span><span class="p">])</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">ys</span><span class="p">[</span><span class="n">t</span><span class="p">]))</span> <span class="c1"># probabilities for next chars</span>
    <span class="n">loss</span> <span class="o">+=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">ps</span><span class="p">[</span><span class="n">t</span><span class="p">][</span><span class="n">targets</span><span class="p">[</span><span class="n">t</span><span class="p">],</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># softmax (cross-entropy loss)</span>
  <span class="c1"># backward pass: compute gradients going backwards</span>
  <span class="n">dWxh</span><span class="p">,</span> <span class="n">dWhh</span><span class="p">,</span> <span class="n">dWhy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Wxh</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Whh</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Why</span><span class="p">)</span>
  <span class="n">dbh</span><span class="p">,</span> <span class="n">dby</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">bh</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">by</span><span class="p">)</span>
  <span class="n">dhnext</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">hs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
  <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">xrange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">))):</span>
    <span class="n">dy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">ps</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>
    <span class="n">dy</span><span class="p">[</span><span class="n">targets</span><span class="p">[</span><span class="n">t</span><span class="p">]]</span> <span class="o">-=</span> <span class="mi">1</span> <span class="c1"># backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here</span>
    <span class="n">dWhy</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dy</span><span class="p">,</span> <span class="n">hs</span><span class="p">[</span><span class="n">t</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">dby</span> <span class="o">+=</span> <span class="n">dy</span>
    <span class="n">dh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Why</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dy</span><span class="p">)</span> <span class="o">+</span> <span class="n">dhnext</span> <span class="c1"># backprop into h</span>
    <span class="n">dhraw</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">hs</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="n">hs</span><span class="p">[</span><span class="n">t</span><span class="p">])</span> <span class="o">*</span> <span class="n">dh</span> <span class="c1"># backprop through tanh nonlinearity</span>
    <span class="n">dbh</span> <span class="o">+=</span> <span class="n">dhraw</span>
    <span class="n">dWxh</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dhraw</span><span class="p">,</span> <span class="n">xs</span><span class="p">[</span><span class="n">t</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">dWhh</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dhraw</span><span class="p">,</span> <span class="n">hs</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">dhnext</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Whh</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dhraw</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">dparam</span> <span class="ow">in</span> <span class="p">[</span><span class="n">dWxh</span><span class="p">,</span> <span class="n">dWhh</span><span class="p">,</span> <span class="n">dWhy</span><span class="p">,</span> <span class="n">dbh</span><span class="p">,</span> <span class="n">dby</span><span class="p">]:</span>
    <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">dparam</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">dparam</span><span class="p">)</span> <span class="c1"># clip to mitigate exploding gradients</span>
  <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">dWxh</span><span class="p">,</span> <span class="n">dWhh</span><span class="p">,</span> <span class="n">dWhy</span><span class="p">,</span> <span class="n">dbh</span><span class="p">,</span> <span class="n">dby</span><span class="p">,</span> <span class="n">hs</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span></code></pre></td></tr></table>
</div>
</div>
<p><strong>Sampling</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">seed_ix</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
  <span class="s2">&#34;&#34;&#34;
</span><span class="s2">  sample a sequence of integers from the model
</span><span class="s2">  h is memory state, seed_ix is seed letter for first time step
</span><span class="s2">  &#34;&#34;&#34;</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">vocab_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
  <span class="n">x</span><span class="p">[</span><span class="n">seed_ix</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
  <span class="n">ixes</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Wxh</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Whh</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span> <span class="o">+</span> <span class="n">bh</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Why</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span> <span class="o">+</span> <span class="n">by</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
    <span class="n">ix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">vocab_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">x</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">ixes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">ixes</span></code></pre></td></tr></table>
</div>
</div>
<p><strong>Gradient Check</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># gradient checking</span>
<span class="kn">from</span> <span class="nn">random</span> <span class="kn">import</span> <span class="n">uniform</span>
<span class="k">def</span> <span class="nf">gradCheck</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">hprev</span><span class="p">):</span>
  <span class="k">global</span> <span class="n">Wxh</span><span class="p">,</span> <span class="n">Whh</span><span class="p">,</span> <span class="n">Why</span><span class="p">,</span> <span class="n">bh</span><span class="p">,</span> <span class="n">by</span>
  <span class="n">num_checks</span><span class="p">,</span> <span class="n">delta</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">1e-5</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">dWxh</span><span class="p">,</span> <span class="n">dWhh</span><span class="p">,</span> <span class="n">dWhy</span><span class="p">,</span> <span class="n">dbh</span><span class="p">,</span> <span class="n">dby</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">lossFun</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">hprev</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">param</span><span class="p">,</span><span class="n">dparam</span><span class="p">,</span><span class="n">name</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="n">Wxh</span><span class="p">,</span> <span class="n">Whh</span><span class="p">,</span> <span class="n">Why</span><span class="p">,</span> <span class="n">bh</span><span class="p">,</span> <span class="n">by</span><span class="p">],</span> <span class="p">[</span><span class="n">dWxh</span><span class="p">,</span> <span class="n">dWhh</span><span class="p">,</span> <span class="n">dWhy</span><span class="p">,</span> <span class="n">dbh</span><span class="p">,</span> <span class="n">dby</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;Wxh&#39;</span><span class="p">,</span> <span class="s1">&#39;Whh&#39;</span><span class="p">,</span> <span class="s1">&#39;Why&#39;</span><span class="p">,</span> <span class="s1">&#39;bh&#39;</span><span class="p">,</span> <span class="s1">&#39;by&#39;</span><span class="p">]):</span>
    <span class="n">s0</span> <span class="o">=</span> <span class="n">dparam</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">s1</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">assert</span> <span class="n">s0</span> <span class="o">==</span> <span class="n">s1</span><span class="p">,</span> <span class="s1">&#39;Error dims dont match: </span><span class="si">%s</span><span class="s1"> and </span><span class="si">%s</span><span class="s1">.&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="sb">`s0`</span><span class="p">,</span> <span class="sb">`s1`</span><span class="p">)</span>
    <span class="k">print</span> <span class="n">name</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">num_checks</span><span class="p">):</span>
      <span class="n">ri</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">))</span>
      <span class="c1"># evaluate cost at [x + delta] and [x - delta]</span>
      <span class="n">old_val</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">flat</span><span class="p">[</span><span class="n">ri</span><span class="p">]</span>
      <span class="n">param</span><span class="o">.</span><span class="n">flat</span><span class="p">[</span><span class="n">ri</span><span class="p">]</span> <span class="o">=</span> <span class="n">old_val</span> <span class="o">+</span> <span class="n">delta</span>
      <span class="n">cg0</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">lossFun</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">hprev</span><span class="p">)</span>
      <span class="n">param</span><span class="o">.</span><span class="n">flat</span><span class="p">[</span><span class="n">ri</span><span class="p">]</span> <span class="o">=</span> <span class="n">old_val</span> <span class="o">-</span> <span class="n">delta</span>
      <span class="n">cg1</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">lossFun</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">hprev</span><span class="p">)</span>
      <span class="n">param</span><span class="o">.</span><span class="n">flat</span><span class="p">[</span><span class="n">ri</span><span class="p">]</span> <span class="o">=</span> <span class="n">old_val</span> <span class="c1"># reset old value for this parameter</span>
      <span class="c1"># fetch both numerical and analytic gradient</span>
      <span class="n">grad_analytic</span> <span class="o">=</span> <span class="n">dparam</span><span class="o">.</span><span class="n">flat</span><span class="p">[</span><span class="n">ri</span><span class="p">]</span>
      <span class="n">grad_numerical</span> <span class="o">=</span> <span class="p">(</span><span class="n">cg0</span> <span class="o">-</span> <span class="n">cg1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">delta</span> <span class="p">)</span>
      <span class="n">rel_error</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">grad_analytic</span> <span class="o">-</span> <span class="n">grad_numerical</span><span class="p">)</span> <span class="o">/</span> <span class="nb">abs</span><span class="p">(</span><span class="n">grad_numerical</span> <span class="o">+</span> <span class="n">grad_analytic</span><span class="p">)</span>
      <span class="k">print</span> <span class="s1">&#39;</span><span class="si">%f</span><span class="s1">, </span><span class="si">%f</span><span class="s1"> =&gt; </span><span class="si">%e</span><span class="s1"> &#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">grad_numerical</span><span class="p">,</span> <span class="n">grad_analytic</span><span class="p">,</span> <span class="n">rel_error</span><span class="p">)</span>
      <span class="c1"># rel_error should be on order of 1e-7 or less</span></code></pre></td></tr></table>
</div>
</div>
<p><strong>Results</strong></p>

<p>Using Shakespeare&rsquo;s sonnet as input:</p>

<p><img src="https://www.yuthon.com/images/RNN_text_generator.png" alt="RNN_text_generator" /></p>

<h3 id="example-image-captioning">Example: Image Captioning</h3>

<p>We use CNN to recognize objects and use RNN to generate captions.</p>

<p><img src="https://www.yuthon.com/images/RNN_image_captioning_structure.png" alt="image_captioning_structure" /></p>

<p>Cut the last two layers from CNN and connect it to RNN:</p>

<p><img src="https://www.yuthon.com/images/RNN_image_captioning_structure_1.png" alt="image_captioning_structure_1" /></p>

<p>And smaple the output from previous layer to next layer as input:</p>

<p><img src="https://www.yuthon.com/images/RNN_image_captioning_structure_2.png" alt="image_captioning_structure_2" /></p>

<p>Sampling is stoped when meeting an END</p>

<p><img src="https://www.yuthon.com/images/RNN_image_captioning_structure_3.png" alt="image_captioning_structure_3" /></p>

<p>Finally, we&rsquo;ll get a complete sentence (using <a href="http://mscoco.org">Microsoft COCO dataset</a>). The first row are good, but the second row may be not satisfactory.</p>

<p><img src="https://www.yuthon.com/images/RNN_image_captioning_result.png" alt="image_captioning_result" /></p>

<blockquote>
<p><strong>Reference</strong>:</p>

<ul>
<li>Explain Images with Multimodal Recurrent Neural Networks, Mao et al.</li>
<li>Deep Visual-Semantic Alignments for Generating Image Descriptions, Karpathy and Fei-Fei</li>
<li>Show and Tell: A Neural Image Caption Generator, Vinyals et al.</li>
<li>Long-term Recurrent Convolutional Networks for Visual Recognition andDescription, Donahue et al.</li>
<li>Learning a Recurrent Visual Representation for Image CaptionGeneration, Chen and Zitnick</li>
</ul>
</blockquote>

<h3 id="more-examples">More examples</h3>

<p>We can also use RNN to generate open source textbooks written in LaTex, or generate C code from Linux source code, or searching for interpretable cells.</p>

<h2 id="long-short-term-memory-lstm">Long Short Term Memory (LSTM)</h2>

<h3 id="vanishing-exploding-gradients">Vanishing/Exploding gradients</h3>

<ul>
<li>Exploding gradients

<ul>
<li>Truncated BPTT</li>
<li><strong>Clip gradients at threshold</strong> (something like anti-windup in control science LOL)</li>
<li>RMSProp to adjust learning rate</li>
</ul></li>
<li>Vanishing gradients

<ul>
<li>Harder to detect</li>
<li>Weight Initialization</li>
<li>ReLU activation functions</li>
<li>RMSProp</li>
<li><strong>LSTM, GRUs</strong> (&lt;&ndash; That&rsquo;s why we use LSTM)</li>
</ul></li>
</ul>

<h3 id="introduction-1">Introduction</h3>

<p>LSTM is proposed in [Hochreiter et al., 1997]. GRU is a knid of simplified LSTM.</p>

<p><img src="https://www.yuthon.com/images/LSTM_diagram.png" alt="LSTM_diagram" /></p>

<blockquote>
<p>ResNet is to PlainNet what LSTM is to RNN, kind of.</p>

<p><img src="https://www.yuthon.com/images/plainnet_vs_resnet.png" alt="plainnet_vs_resnet" /></p>
</blockquote>

<h3 id="concept-1">Concept</h3>

<p><img src="https://www.yuthon.com/images/LSTM_structure.png" alt="LSTM_structure" /></p>

<p>LSTM have two states, one is <strong>cell state</strong> ($c$), another is <strong>hidden state</strong> ($h$):</p>

<ul>
<li>$i$: input gate, &ldquo;add to memory&rdquo;, decides whether do we want to add value to this cell.</li>
<li>$f$: forget gate, &ldquo;flush the memory&rdquo;, decides whether to shut off the cell and reset the counter.</li>
<li>$o$: output gate, &ldquo;get from memory&rdquo;, decides how much do we want to get from this cell.</li>
<li>$g$: input, decides how much do we want to add to this cell.</li>
</ul>

<h2 id="summary">Summary</h2>

<ul>
<li>RNNs allow a lot of flexibility inarchitecture design</li>
<li>Vanilla RNNs are simple but don’twork very well</li>
<li>Common to use LSTM or GRU: theiradditive interactions improve gradient flow</li>
<li>Backward flow of gradients in RNNcan explode or vanish. Exploding is controlled with gradient clipping.Vanishing is controlled with additive interactions (LSTM)</li>
<li>Better/simpler architectures are ahot topic of current research</li>
<li>Better understanding (boththeoretical and empirical) is needed.</li>
</ul>

<p>(To be improved by adding extra materials&hellip;)</p>

<h1 id="references">References</h1>

<ul>
<li>[<a href="https://zhuanlan.zhihu.com/p/22107715?refer=intelligentunit">原创翻译]循环神经网络惊人的有效性（上）</a></li>
<li>[<a href="https://zhuanlan.zhihu.com/p/22230074?refer=intelligentunit">原创翻译]循环神经网络惊人的有效性（下）</a></li>
<li><a href="https://www.youtube.com/watch?v=Ukgii7Yd_cU">Recurrent Neural Networks (Video, recommend)</a></li>
<li><a href="http://james371507.wixsite.com/hylee/single-post/2016/03/20/Recurrent-Neural-Network-RNN-Note-of-Stanford-CS231n">Recurrent Neural Network (RNN) (Note of Stanford CS231n)</a></li>
</ul>

    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">Author</span>
    <span class="item-content">Yusu Pan</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">LastMod</span>
    <span class="item-content">
        2016-10-30
        
    </span>
  </p>
  
  <p class="copyright-item">
    <span class="item-title">License</span>
    <span class="item-content"><a rel="license noopener" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank">CC BY-NC-ND 4.0</a></span>
  </p>
</div>
<footer class="post-footer">
      <div class="post-tags">
          <a href="https://www.yuthon.com/tags/cs231n/">CS231n</a>
          <a href="https://www.yuthon.com/tags/recurrent-neural-network/">Recurrent Neural Network</a>
          <a href="https://www.yuthon.com/tags/deep-learning/">Deep Learning</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="https://www.yuthon.com/post/papers/notes-for-slic/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">Notes for SLIC</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        <a class="next" href="https://www.yuthon.com/post/projects/traffic-prediction-using-lstm/">
            <span class="next-text nav-default">Traffic Prediction Using LSTM</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        <div id="disqus_thread"></div>
    <script type="text/javascript">
    (function() {
      
      
      if (window.location.hostname === 'localhost') return;

      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      var disqus_shortname = 'yuthons-blog';
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:xxdsox@gmail.com" class="iconfont icon-email" title="email"></a>
      <a href="https://stackoverflow.com/users/5682817" class="iconfont icon-stack-overflow" title="stack-overflow"></a>
      <a href="https://twitter.com/corenel" class="iconfont icon-twitter" title="twitter"></a>
      <a href="https://www.facebook.com/xxdsox" class="iconfont icon-facebook" title="facebook"></a>
      <a href="https://github.com/corenel" class="iconfont icon-github" title="github"></a>
      <a href="https://www.zhihu.com/people/pan-yu-su" class="iconfont icon-zhihu" title="zhihu"></a>
  <a href="https://www.yuthon.com/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2017 - 
    2019
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">© This post is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License, please give source if you wish to quote or reproduce.
</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>
<script type="text/javascript" src="https://www.yuthon.com/dist/even.26188efa.min.js"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      TeX: {equationNumbers: {autoNumber: "AMS"}},
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script type="text/javascript" async src="https://www.yuthon.com/lib/mathjax/math-code.js"></script>
    <script async src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"  integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script>


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-76233259-1', 'auto');
	ga('set', 'anonymizeIp', true);
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>







</body>
</html>
