<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>Something about GAN - Yuthon&#39;s Blog</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="Yusu Pan" /><meta name="description" content="最近在看关于GANs的论文，并且自己动手用PyTorch写了一些经典文章的实现，想要稍微总结一下，故有此文。在最后我总结了我自己看过的有关GANs的一些比较好的资源，希望对读者有所帮助。
" /><meta name="keywords" content="yuthon, yusu-pan, blog, deep-learning, computer-vision" />






<meta name="generator" content="Hugo 0.54.0 with even 4.0.0" />


<link rel="canonical" href="https://www.yuthon.com/post/tutorials/something-about-gans/" />
<link rel="apple-touch-icon" sizes="180x180" href="https://www.yuthon.com/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://www.yuthon.com/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://www.yuthon.com/favicon-16x16.png">
<link rel="manifest" href="https://www.yuthon.com/manifest.json">
<link rel="mask-icon" href="https://www.yuthon.com/safari-pinned-tab.svg" color="#5bbad5">


<link href="https://www.yuthon.com/dist/even.c2a46f00.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="Something about GAN" />
<meta property="og:description" content="最近在看关于GANs的论文，并且自己动手用PyTorch写了一些经典文章的实现，想要稍微总结一下，故有此文。在最后我总结了我自己看过的有关GANs的一些比较好的资源，希望对读者有所帮助。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.yuthon.com/post/tutorials/something-about-gans/" />
<meta property="article:published_time" content="2017-08-12T19:24:14&#43;00:00"/>
<meta property="article:modified_time" content="2017-08-12T19:24:14&#43;00:00"/>

<meta itemprop="name" content="Something about GAN">
<meta itemprop="description" content="最近在看关于GANs的论文，并且自己动手用PyTorch写了一些经典文章的实现，想要稍微总结一下，故有此文。在最后我总结了我自己看过的有关GANs的一些比较好的资源，希望对读者有所帮助。">


<meta itemprop="datePublished" content="2017-08-12T19:24:14&#43;00:00" />
<meta itemprop="dateModified" content="2017-08-12T19:24:14&#43;00:00" />
<meta itemprop="wordCount" content="13989">



<meta itemprop="keywords" content="Generative Adversarial Networks,GANs,DCGAN,WGAN,WGAN-GP,Deep Learning," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Something about GAN"/>
<meta name="twitter:description" content="最近在看关于GANs的论文，并且自己动手用PyTorch写了一些经典文章的实现，想要稍微总结一下，故有此文。在最后我总结了我自己看过的有关GANs的一些比较好的资源，希望对读者有所帮助。"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="https://www.yuthon.com/" class="logo">Yuthon&#39;s Blog</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="https://www.yuthon.com/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="https://www.yuthon.com/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a><a href="https://www.yuthon.com/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="https://www.yuthon.com/about/">
        <li class="mobile-menu-item">About</li>
      </a><a href="https://www.yuthon.com/resume/">
        <li class="mobile-menu-item">Resume</li>
      </a><a href="https://github.com/corenel">
        <li class="mobile-menu-item">Works</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="https://www.yuthon.com/" class="logo">Yuthon&#39;s Blog</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="https://www.yuthon.com/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="https://www.yuthon.com/categories/">Categories</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="https://www.yuthon.com/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="https://www.yuthon.com/about/">About</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="https://www.yuthon.com/resume/">Resume</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="https://github.com/corenel">Works</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">Something about GAN</h1>

      <div class="post-meta">
        <span class="post-time"> 2017-08-12 </span>
        <div class="post-category">
            <a href="https://www.yuthon.com/categories/notes/"> Notes </a>
            <a href="https://www.yuthon.com/categories/tutorials/"> Tutorials </a>
            </div>
          <span class="more-meta"> 13989 words </span>
          <span class="more-meta"> 28 mins read </span>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
<ul>
<li>
<ul>
<li><a href="#before-reading-pytorch">Before Reading: PyTorch</a></li>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#how-do-gans-work">How do GANs Work?</a>
<ul>
<li><a href="#the-gan-framework">The GAN framework</a></li>
<li><a href="#cost-function">Cost function</a></li>
</ul></li>
<li><a href="#dcgan">DCGAN</a></li>
<li><a href="#wgan-wgan-gp">WGAN &amp; WGAN-GP</a>
<ul>
<li><a href="#what-does-it-mean-to-learn-a-probability-distribution">What does it mean to learn a probability distribution?</a></li>
<li><a href="#different-disrances">Different Disrances</a></li>
<li><a href="#wasserstein-gan">Wasserstein GAN</a></li>
<li><a href="#wgan-gp">WGAN-GP</a></li>
</ul></li>
<li><a href="#gans-in-practice">GANs in Practice</a>
<ul>
<li><a href="#mlp-gan">MLP-GAN</a></li>
<li><a href="#dcgan-1">DCGAN</a></li>
<li><a href="#wgan">WGAN</a></li>
<li><a href="#wgan-gp-1">WGAN-GP</a></li>
</ul></li>
<li><a href="#further-reading">Further Reading</a></li>
</ul></li>
</ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <p>最近在看关于GANs的论文，并且自己动手用PyTorch写了一些经典文章的实现，想要稍微总结一下，故有此文。在最后我总结了我自己看过的有关GANs的一些比较好的资源，希望对读者有所帮助。</p>

<h2 id="before-reading-pytorch">Before Reading: PyTorch</h2>

<p>在讲GANs之前，首先推一波PyTorch。就我的使用体验来说，PyTorch是远远超过TensorFlow的。PyTorch作为一个动态图计算的框架，与Python结合得非常好，写出来的代码非常*Pythonic*（反例就是TF的<code>tf.while_loop</code>）。同时PyTorch与NumPy结合得非常好，不用在<code>Tensor</code>与<code>ndarray</code>之间转换来转换去。</p>

<p>总而言之，PyTorch非常适合我这样需要快速开发与快速验证，并且对于运行速度要求并不高的DL研究（讲真，TF的速度也不怎么快，还是吃内存大户）。</p>

<h2 id="introduction">Introduction</h2>

<p><strong>GANs（Generative Adversarial Networks，生成对抗网络）是Generative Models（生成模型）的一种。</strong>所谓生成模型，就是能在一个含有真实数据分布$p_{data}$样本（samples）的训练集上，学习到真实数据分布的估计的表示$p_{model}$的模型。学习到的这个表示可以是显式的（explicit），比如说直接就给出了$p_{model}$；也可以是隐式的（implicit），比如说能够生成符合$p_{model}$分布的样本。<strong>一般来说，GANs属于第二种，能够进行样本生成。</strong>不过在设计上，GANs其实是两者皆可的。</p>

<p><strong>那为什么要搞这个生成模型呢？</strong>原因也是有很多的。比如说，训练生成样本是一个用来测试我们表示高维概率分布的能力的好方法，而现实世界的物体往往是具有高维概率分布的，这就对我们用DL来表示与解释现实世界有帮助。同时，很多DL相关的任务也是很需要样本生成的，比如说超分辨率（super resolution）、风格迁移（style transfer）之类的。此外，生成模型对于某些标签甚至是数据缺失的数据集也很有用，比如说可以用在半监督学习（semi-supervised learning）上，不过我倒是没见到过相关的论文。总之，生成模型是很有用的，也是目前DL领域的一大热门方向。</p>

<p>那么问题又来了，<strong>还有哪些生成模型，GANs相比于其他的生成模型有什么优势</strong>，特别是在去年和它几乎同时火起来的VAE？首先谈谈生成模型的分类。说起这个，就不得不谈<strong>极大似然估计（maximum likelihood estimation）</strong>，几乎所有的生成模型都使用了或者可以转换为使用极大似然来进行估计。极大似然估计的基本思想是，定义一个含有参数$\theta$的模型，用它来提供对于一个概率分布的估计，然后寻找一组最优的参数$\theta$来使得模型的概率分布$p_{model}$最贴合实际数据的概率分布$p_{data}$。具体地说，对于一个具有$m$个样本$x^{(i)}$的数据集来说，可以用似然（likelihood）来表示模型与数据集中数据的契合概率$\prod^m_{i=1} p_{model} (x^{(i)};\theta)$，而我们的最终目的就是找到使得似然最大的参数$\theta$。由于对数函数的优良性质，我们可以将似然取对数。
$$
\begin{align}
\theta ^* &amp;= \underset{\theta}{\operatorname{argmax}} \prod ^m _{i=1} p_{model} (x^{(i)};\theta) \<br />
&amp;= \underset{\theta}{\operatorname{argmax}} \log \prod ^m _{i=1} p_{model} (x^{(i)};\theta) \<br />
&amp;= \underset{\theta}{\operatorname{argmax}} \sum ^m _{i=1} \log p_{model} (x^{(i)};\theta) \<br />
\end{align}
$$</p>

<p>另外一方面，我们也可以将极大似然估计看成是一种最小化真实概率分布与模型概率分布之间的<strong>KL散度（Kullback–Leibler divergence）</strong>的方法。虽然我们通常在实践中并不能直接接触到$p_{data}$，而是只能够获得服从其分布的$m$个采样。我们可以用这些采样组成的数据集来定义$\hat{p} _{data}$这么一个经验分布（empirical distribution）来近似$p_{data}$。可以证明，最小化$\hat{p} _{data}$与$p _{model}$之间的KL散度，等价于在数据集上最大化对数似然。
$$
\theta ^*= \underset{\theta}{\operatorname{argmin}} D_{KL} (p_{data} (x) \parallel p_{model} (x;\theta))
$$</p>

<p>好了，说了这么多，让我们再把话题转向生成模型的分类。生成模型可以根据其计算似然及其梯度，或者近似估计这些值的方法来进行分类。其中值得注意的有FBVNs、VAE以及GANs。GANs属于右边的那一类，能够隐式地得到概率密度，并直接从中生成样本。</p>

<p><img src="https://www.yuthon.com/images/GAN_taxonomy_for_generative_models.jpg" alt="taxonomy_for_generative_models" /></p>

<p>GANs相对于其他的生成模型，其优点主要在于：</p>

<ul>
<li>GANs能够并行地生成样本，而非FBVNs那样只能串行；</li>
<li>GANs在设计上的约束很少，不像玻尔兹曼机（Boltzmann Machine）那样只能用少数几种概率分布，也不像非线性ICA那样，要求生成器必须可逆并且隐式编码（latent code）$z$必须与数据集中的样本$x$具有相同的维度；</li>
<li>GANs不需要马尔科夫链（Markov chains），这点不同于玻尔兹曼机以及GSNs；</li>
<li>GANs不需要使用微分边界（variational bound），并且GANs里面用到的模型早已被证明是万能逼近器（universial approximators），因此GANs能够保证渐进一致（asymptotically consistent）。相比而言，某些VAEs虽然推测是渐进一致的，但是没有得到证明；</li>
<li>最后一点，GANs就目前的效果来说，其生成出来的样本的质量比用其他生成模型得到的要好。</li>
</ul>

<p>当然，原始的GANs有一点是非常让人头疼的，就是它的训练过程本质上是寻找一场比赛的纳什均衡（Nash equilibrium）的过程，这导致GANs很难稳定的训练。当然，之后要提到的WGAN在一定程度上解决了这问题。</p>

<h2 id="how-do-gans-work">How do GANs Work?</h2>

<h3 id="the-gan-framework">The GAN framework</h3>

<p>GANs由两个部分组成：一个是<strong>生成器（generator）</strong>，负责生成样本，并且尽力与原始数据集中的分布一致；另一个是<strong>判别器（discriminator）</strong>，负责检验输入的样本是来自真实数据分布还是生成器生成的。这两者都可以表现为可微的函数（其实最后就是表现为神经网络），生成器是以$z$为输入，$\theta^{(G)}$为参数的函数$G$，而判别器是以$x$为输入，$\theta^{(D)}$为参数的函数$D$。生成器的目的是在只能控制$\theta^{(G)}$的情况下，最小化$J^{(G)} (\theta ^{(D)}, \theta ^{(G)})$；而判别器则是在只能改变$\theta^{(D)}$的情况下，最小化$J^{(D)} (\theta ^{(D)}, \theta ^{(G)})$。通俗地说就是，生成器想要生成出的样本能够让判别器区分不出这是来自真实数据还是生成的（$D(G(z))=1$），而判别器则是想要尽可能地将这些区分开来（$D(G(z))=0$）。这么一来，这场比赛的<strong>解就是一个纳什均衡</strong>。也就是说，这个解$(\theta ^{(D)}, \theta ^{(G)})$不但能在$J^{(G)}$上取到局部最小值（local minimum），而且在$J^{(D)}$上也取到局部最小值。如果两者均具有足够的容量（capacity），则$\forall x, D(x)=D(G(z))=\frac{1}{2}$。</p>

<p><img src="https://www.yuthon.com/images/GAN_framework.jpg" alt="the-GAN-framework" /></p>

<p>生成器就是一个简单的可微分的函数$G$，一般来说我们用DNN来表示。生成器接受一个来自先验分布（比如说高斯分布）的采样$z$，然后对其进行处理，得到一个来自$p_{model}$分布的采样$G(z)$。值得注意的是，我们并不一定要把$z$作为DNN第一层的输入。比如说我们可以把$z$一刀切成两部分，$z^{(1)}$和$z^{(2)}$，$z^{(1)}$作为DNN首层的输入，而$z^{(2)}$则加在DNN末层的输出上。还有一种操作是，我们可以在DNN的隐藏层上搞加性噪声（additive noise）或者乘性噪声（multiplicative noise），或者直接在隐藏层输出上串（concatenate）一个噪声。总而言之，生成器网络在设计上的约束是很少的，可以任意开脑洞，只要<strong>保证$z$的维度不低于$x$，并且整个网络是可微的</strong>就可以了。</p>

<p>判别器就是一个简单的二分类的DNN，在此就不赘述了。</p>

<h3 id="cost-function">Cost function</h3>

<p><strong>判别器的代价函数就是简单的交叉熵（cross-entropy cost）</strong>，如下所示。唯一的区别是，判别器的一次训练由两个mini-batches组成，一部分是来自于数据集的真实样本，标签为1；另一部分是来自生成器所生成的样本，标签为0。一般来说，所有的GANs的判别器都是用下述公式作为代价函数的。
$$
J^{(D)} (\theta ^{(D)}, \theta ^{(G)}) = -\frac{1}{2} \mathbb{E}_{x\sim p_{data}} \log D(x) - \frac{1}{2} \mathbb{E}_{z} \log (1-D(G(z)))
$$
生成器的代价函数的可选择范围就稍微多了一些，主要有minimax、heuristic以及maximum likelihood三种，其中前两种比较常见。</p>

<ul>
<li><p><strong>Minimax</strong>：其实就是零和游戏（zero-sum game），生成器的代价函数等于判别器代价函数的负值。可以证明，这等价于最小化数据与模型之间的JS散度（Jenson-Shannon divergence）。但是这个代价函数其实是存在着隐患的。一般来说，判别器训练的收敛速度比生成器快得多，因此判别器很快就能以较高的置信度将生成器生成的假样本给拒绝掉，从而造成生成器的梯度消失的问题。这一缺陷可以用下面的方法来解决。
$$
J^{(G)} = -J^{(D)} \<br />
V (\theta ^{(D)}, \theta ^{(G)}) = J^{(D)} (\theta ^{(D)}, \theta ^{(G)}) \<br />
\theta ^{(G)*} = \underset{\theta ^{(G)}}{\operatorname{argmin}} \underset{\theta ^{(D)}}{\operatorname{max}} V (\theta ^{(D)}, \theta ^{(G)})
$$</p></li>

<li><p><strong>Heuristic, non-saturating game</strong>：我们可以换种方式来思考问题：在minimax game中，我们是让生成器最小化判别器判对的对数概率，这会导致一些问题；那么可不可以让生成器来最大化判别器判错的对数概率呢？显然也是可以的，并且这也能避免生成器梯度消失的问题。这是一种启发式的方法，其动机是为了让玩家（也就是生成器）在“输掉”游戏的时候能得到比较强的梯度。
$$
J^{(G)} = - \frac{1}{2} \mathbb{E}_{z} \log (D(G(z)))
$$</p></li>
</ul>

<p>从下图可以看出，heuristicly designed non-saturating cost在$D(G(z))$变化的时候，其方差较小，因此是比较合适作为生成器代价函数的选择的。</p>

<p><img src="https://www.yuthon.com/images/GAN_cost_functions.jpg" alt="cost_functions_of_GANs" /></p>

<h2 id="dcgan">DCGAN</h2>

<p>DCGAN即使用了全卷积网络的GANs，一般特指<a href="https://arxiv.org/abs/1511.06434">这篇paper</a>中的网络结构。目前几乎所有的GANs都或多或少地借鉴了DCGAN的架构。DCGAN的主要创新点在于：</p>

<ul>
<li><strong>同时在判别器与生成器网络中使用了Batch Normalization层。</strong>当然，为了能够学到真实数据分布正确的均值（mean）与规模（scale），判别器的首层与生成器的末层没有加BN层。</li>
<li><strong>整个网络架构借鉴了the all-convolutional net</strong>（不是FCN），不含pooling和“unpooling”层，增加表示维度是靠<code>stride=1</code>的转置卷积（transposed convolition）实现的。</li>
</ul>

<p>总而言之，就是换了原始的GAN中的网络架构，把FC层都换成了带BN层的卷积层。</p>

<h2 id="wgan-wgan-gp">WGAN &amp; WGAN-GP</h2>

<h3 id="what-does-it-mean-to-learn-a-probability-distribution">What does it mean to learn a probability distribution?</h3>

<p>WGAN在<a href="https://arxiv.org/abs/1701.07875">paper</a>开头就直截了当地提出了一个问题，<strong>我们怎么样才算是学到了一个概率分布呢？</strong>如果按照本文开头的说法，我们的做法是先定义一个参数化的概率密度族$(P_\theta )_{\theta \in \mathbb{R}}$，然后通过在已有的数据集${x^{(i)}} ^m_{i=1}$上最大化似然的方法来找到一个最佳的参数，并将这个参数对应的那个概率密度$P_{\theta}​$视为我们所学习到的模型。
$$
\underset{\theta \in \mathbb{R}}{\max} \frac{1}{m} \sum ^m_{i=1} \log P_{\theta} (x^{(i)})
$$
如果真实的数据分布$\mathbb{P}_r$存在密度，而我们学习到的概率密度$P_{\theta}$所对应的概率分布为$\mathbb{P}_{\theta}$，则<strong>上述学习过程实际上等价于最小化$\mathbb{P}_r$与$\mathbb{P}_{\theta}$之间的KL散度$KL(\mathbb{P}_r \parallel \mathbb{P}_{\theta})$。</strong>这也是上文提到过的。</p>

<p>是不是看起来有理有据令人信服？但是，这上面的推理过程有一个很重要很显而易见但是又常常为人所忽视的条件，那就是<strong>首先$\mathbb{P}_r$与$\mathbb{P}_{\theta}$之间的KL散度必须存在，然后我们才能去最小化这个KL散度，来获取最优$\theta$</strong>。你TM在逗我，KL散度还能不存在？事实上，这是很常见的。比如说我们用GANs生成样本的时候，输入的随机噪声通常就是一个64维的向量，然后经过生成器网络来生成一个$64\times64=4096$维的图片，其本质还是决定于开始的那个64维的随机向量。64维相对于4096维来说实在是微不足道，<strong>也就是说，生成样本分布的支撑集（support）构成了高维空间上的低维流形（manifold），撑不满整个高维空间。那么该模型的流形与真实分布的支撑集之间很有可能就没有不可忽视的交集（non-negligible intersection），以致两者间的KL散度不存在，或者说是$\infty$。</strong></p>

<p>KL散度都不存在了，那还优化个毛？不过这难不倒千千万万机智的研究者。<strong>你不是要让这两个概率分布有交集吗，那我就往$\mathbb{P}_{\theta}​$上加噪声，加个大点的噪声总是能让这两个分布碰在一起的，那KL散度不是就有了？</strong>这也是几乎所有的生成模型都包含了噪声项的原因。一般来说，在训练开始的时候加的噪声要大一点，让含有噪声的两个分布能够够得着。随着训练过程的深入，两个分布的主体开始慢慢靠近，这时候噪声就需要慢慢减小了。最后两个分布开始真正有了不可忽视的交集，那么此时加不加噪声也没什么关系了。</p>

<p>不过这种方法也是存在着一些问题的，加噪声很可能使得生成出来的样本比较模糊（blurry）。如果加的是<code>mean=0.1</code>的高斯噪声，而像素值本身又已经归一化到了$[0,1]$，那么很显然，这个噪声相对于像素值来说太大了。于是机智的研究者在论文里展示生成的样本的时候，不像他们在训练过程中干的那样，是不怎么加噪声的。换句话说，加噪声虽然在一定程度上保证了KL散度的存在，也就是使得极大似然方法能够奏效，但是这对于问题本质上来说并没有改善，并非解决问题的正确方法（走上了邪路^_^）。</p>

<p>与其显式地估计有可能并不存在的$\mathbb{P}_r$的密度，不如<strong>直接将来自先验分布$p(z)$的随机变量$\mathcal{Z}$通过参数化的映射$g_\theta : \mathcal{Z} \to \mathcal{X}$来生成样本$\mathcal{X}$，只要这个映射出来的样本$\mathcal{X}$的分布服从或者接近$\mathbb{P}_r$</strong>，那不就是学到了真实的概率分布？VAEs和GANs就是这么做的。这么做有两个好处，首先其与直接估计密度的方法不同，能够表示被约束在低维流形的概率分布；同时直接生成样本有时候比得到一个干巴巴的概率密度更加有用。</p>

<p>那么问题又转移到了，<strong>如何衡量生成样本分布$\mathbb{P}_{\theta}$与真实样本分布$\mathbb{P}_r$之间的相似性或者说距离$\rho(\mathbb{P}_{\theta}, \mathbb{P}_r)$？</strong>这个距离度量需要有比较好的性质，不能两个分布没有交集就直接歇菜了（说的就是KL散度你这个大坑货）。WGAN论文之后的部分就在讲如何定义一个好的距离度量，并将其应用在GANs中。</p>

<p>各个距离度量的一个基本的差异在于它们对于成序列的概率分布的收敛性的影响。一个概率分布的序列$(\mathbb{P}_t)_{t\in\mathbb{R}}$能够收敛，当且仅当存在一个$\mathbb{P}_\infty$使得$\rho(\mathbb{P}_{\theta}, \mathbb{P}_r)$趋向于零，也就是说取决于$\rho$的定义。一般来说，如果$\rho$在拓扑（topology）上越弱，则序列越容易收敛。</p>

<p>为了优化生成模型的参数$\theta$，我们希望定义的模型能够使得映射$\theta \mapsto \mathbb{P_\theta}$连续。这里的连续指的是当一连串的参数$\theta_t$收敛于一个值$\theta$，概率$\mathbb{P}_{\theta_t}$也能收敛于$\mathbb{P}_\theta$。不过这种连续性取决于我们选择的距离度量。距离度量越弱，概率分布越容易瘦脸，则越容易定义一个从$\theta$空间到$\mathbb{P}_\theta$空间的连续的映射。我们这里关注映射$\theta \mapsto \mathbb{P_\theta}$连续性的原因是，<strong>我们希望$\theta \mapsto \rho(\mathbb{P}_{\theta}, \mathbb{P}_r)$的损失函数是连续的（方便梯度下降训练），而这等价于在使用距离度量$\rho$的情况下映射$\theta \mapsto \mathbb{P_\theta}$连续。</strong></p>

<h3 id="different-disrances">Different Disrances</h3>

<p>令$\mathcal{X}$为紧致的度量集合（例如图像空间$[0,1]^d$），$\Sigma$为$\mathcal{X}$的Borel子集，$\operatorname{Prob}(\mathcal{X})$为$\mathcal{X}$上的概率度量空间，则可以定义两个分布$\mathbb{P}_r, \mathbb{P}_g \in \operatorname{Prob}(\mathcal{X})$之间的距离或是散度如下：</p>

<ul>
<li><p><strong>TV距离（The Total Variation distance）</strong>
$$
\delta (\mathbb{P}_r, \mathbb{P}_g) = \underset{A\in\Sigma}{\sup} |\mathbb{P}_r (A) - \mathbb{P}_g(A)|
$$</p></li>

<li><p><strong>KL散度（The Kullback-Leibler divergence）</strong>：注意KL散度是不对称的，而且在例如$P_g (x) = 0$且$P_r (x) &gt; 0$的情况下会变成无穷。
$$
KL(\mathbb{P}_r \parallel \mathbb{P}_g) = \int \log \left( \frac{P_r (x)}{P_g (x)} \right) P_r (x) d \mu (x)
$$</p></li>

<li><p><strong>JS散度（The Jensen-Shannon divergence）</strong>：其中$\mathbb{P}_m= (\mathbb{P}_r + \mathbb{P}_g) / 2$。JS散度是对称的，并且始终是有定义的。DCGAN的生成器的损失函数就是JS散度形式的，但是JS散度也会导致一些问题。
$$
JS(\mathbb{P}_r, \mathbb{P}_g) = KL(\mathbb{P}_r \parallel \mathbb{P}_m) + KL(\mathbb{P}_g \parallel \mathbb{P}_m)
$$</p></li>

<li><p><strong>EM距离（The Earth-Mover distance or Wasserstein-1）</strong>：其中$\prod(\mathbb{P}_r, \mathbb{P}_g)$表示所有边际概率为$\mathbb{P}_r$与$\mathbb{P}_g$的联合分布$\gamma (x,y)$的集合。直观地说，$\gamma (x,y)$表示了为了将分布$\mathbb{P}_r$变换成$\mathbb{P}_g$所需要的从$x$移到$y$的”质量“的多少。而EM距离则代表了最优搬运方案的代价。
$$
W(\mathbb{P}_r, \mathbb{P}_g) = \underset{\gamma\in\prod(\mathbb{P}_r, \mathbb{P}_g)}{\inf} \mathbb{E} _{(x,y)\sim \gamma} \left[ \parallel x- y\parallel \right]
$$</p></li>
</ul>

<p>为了比较这几个距离度量的优劣，WGAN设计了一个例子：令$Z \sim U[0,1]$，$\mathbb{P}_0$为$(0, Z) \in \mathbb{R}^2$的分布（$x$坐标为$0$，$y$坐标为$Z$，实际上就是分布在点$(0,0)$到点$(0,1)$这条线段上）。令生成样本分布为$g_\theta (z)=(\theta, z)$，其中以$\theta$作为唯一的实值参数（实际上就就是沿着$x$轴左右移动之前提到的线段）。</p>

<p><img src="https://www.yuthon.com/images/WGAN_example_1.jpg" alt="WGAN_example_1" /></p>

<p><img src="https://www.yuthon.com/images/WGAN_figure_1.jpg" alt="WGAN_figure_1" /></p>

<p>则前文所述的各个距离度量的表达式为：</p>

<ul>
<li>$W(\mathbb{P}_\theta, \mathbb{P}_0) = |\theta|$</li>
<li>$JS(\mathbb{P}_\theta, \mathbb{P}_0) =  \begin{cases} \log 2, &amp; \text{if } \theta \ne 0 \\  0, &amp; \text{if } \theta = 0 \end{cases}$</li>
<li>$KL(\mathbb{P}_\theta, \mathbb{P}_0) = (\mathbb{P}_0, \mathbb{P}_\theta) = \begin{cases} +\infty, &amp; \text{if } \theta \ne 0 \\  0, &amp; \text{if } \theta = 0 \end{cases}$</li>
<li>$\delta(\mathbb{P}_\theta, \mathbb{P}_0) = \begin{cases} 1, &amp; \text{if } \theta \ne 0 \\  0, &amp; \text{if } \theta = 0 \end{cases}$</li>
</ul>

<p>可以看出，当$\theta _t \to 0$，只有EM距离能够使得序列$(\mathbb{P}_{\theta _t}) _{t \in N}$收敛于$\mathbb{P}_0$，而其他的JS散度、KL散度、KL散度取反或者TV距离都不能。这说明<strong>在EM距离下，我们能够通过梯度下降的方式习得低维流形上的概率分布，而在其他距离度量下甚至连损失函数的连续性都不能保证，更遑论习得概率分布了。</strong>一句话，EM距离大法好！因此，WGAN将EM距离（又称Wasserstein-1距离）作为两个概率分布之间的距离度量，从而推导损失函数的表达式。</p>

<h3 id="wasserstein-gan">Wasserstein GAN</h3>

<p>从上面的例子可以看出，<strong>Wasserstein距离比JS散度具有更优良的性质</strong>。（当然，WGAN原论文里面还有一大堆数学推导来证明，这里就不列了。）但是，Wasserstein距离的原始公式是很难计算的（intractable）。WGAN里使用了一个Kantorovich-Rubinstein duality的定理，将EM距离的式子转换为：
$$
W(\mathbb{P}_\theta, \mathbb{P}_0) = \frac{1}{K} \underset{\parallel f \parallel _L \le K}{\sup} \mathbb{E}_{x\sim \mathbb{P}_r} - \mathbb{E}_{x\sim \mathbb{P}_\theta} [f(x)]
$$
这里的$f(x)$需要是K-Lipschitz函数，也就是说要求$\exists K\in \mathbb{R},K\ge0$，使得对于实值函数$f(x)$定义域内的任意$x_1$与$x_2$，都满足下式：
$$
|f(x_1)-f(x_2)| \le K |x_1-x_2|
$$
假设我们有一个符合K-Lipschitz条件的函数族${f_w}_{w\in \mathcal{W}}$，那么就变成了解决以下问题：
$$
\underset{w\in\mathcal{W}}{\max} \mathbb{E} _{x\sim \mathbb{P}_r} [f_w (x)] - \mathbb{E} _{z\sim p(z)} [f_w (g_\theta (z))]
$$
这个函数族自然是可以用一个含参$w\in \mathcal{W}$的神经网络来表示，但是如何满足K-Lipschitz条件呢？需要注意的是，整个函数族${f_w}_{w\in \mathcal{W}}$需要符合条件，但我们并不关心$K$的具体值。因此<strong>WGAN论文中提出了一种简单粗暴的做法，把权重的值域$\mathcal{W}$限制在一个非常小的范围内</strong>，比如说$\mathcal{W} = [-0.01, 0.01]$，也就是$w \in [-0.01, 0.01]$。这么一来，$\partial f_w / \partial x$也会被限制在一定范围内，则$\exists K\in \mathbb{R},K\ge0$使得$f_w$满足K-Lipschitz条件。当然这种做法很粗糙，也会导致某些问题，这些问题与解决放将在之后的WGAN-GP小节中详述。</p>

<p>WGAN又做了一个实验，如下图所示，判别器学习区分真假两个高斯分布。可以看出用了JS散度的判别器虽然能很快就把这两个样本分布判别出来，但是在左右两端都处于饱和状态，根本提供不了梯度信息。而<strong>用了Wasserstein距离的判别器（文中称作critic）则几乎能在任何位置都提供线性的梯度信息，这对于生成器的训练来说是非常重要的。</strong></p>

<p><img src="https://www.yuthon.com/images/WGAN_figure_2.jpg" alt="WGAN_figure_2" /></p>

<p>到此为止，WGAN所使用的Wasserstein距离就介绍完了。损失函数定义如下：
$$
\mathcal{L}_D = \mathbb{E} _{z\sim p(z)} [f_w (g_\theta (z))] - \mathbb{E} _{x\sim \mathbb{P}_r} [f_w (x)]   \<br />
\mathcal{L}_G = - \mathbb{E} _{z\sim p(z)} [f_w (g_\theta (z))]
$$
具体到网络训练上，相对于DCGAN的主要改进有：</p>

<ul>
<li>判别器与生成器的loss不用log</li>
<li>判别器不用Sigmoid层</li>
<li>优化器更新参数之后将权重限制在一个非常小的区间$\mathcal{W}$内</li>
<li>使用RMSprop而非基于动量的优化器</li>
<li>在初始阶段多训练判别器几次（$n_{critic}=100$），保证判别器在初始的时候就已经达到一个比较好的水准，增加训练的稳定性</li>
</ul>

<p>WGAN训练过程的伪代码如下所示：</p>

<p><img src="https://www.yuthon.com/images/WGAN_algorithm.jpg" alt="WGAN_algorithm" /></p>

<p><strong>除了改善网络训练的稳定性之外，WGAN所使用的Wasserstein距离还是一个非常好的训练程度的指示器。</strong>但是JS散度就没有这个作用。</p>

<p><img src="https://www.yuthon.com/images/WGAN_figure_3.jpg" alt="WGAN_figure_3" /></p>

<p><img src="https://www.yuthon.com/images/WGAN_figure_4.jpg" alt="WGAN_figure_4" /></p>

<p>使用Wasserstein距离与使用JS散度训练出来的DCGAN，在正常情况下生成样本质量是差不多的：</p>

<p><img src="https://www.yuthon.com/images/WGAN_figure_5.jpg" alt="WGAN_figure_5" /></p>

<p>但是，WGAN还能在生成器去掉BN层之后依然得到比较好的生成样本，原始DCGAN就直接崩了。</p>

<p><img src="https://www.yuthon.com/images/WGAN_figure_6.jpg" alt="WGAN_figure_6" /></p>

<p>此外，如果都用MLP而不是CNN，WGAN的质量会稍差一些，但是使用JS散度训练出来的DCGAN还会有多样性不足（mode collapse）的缺陷。</p>

<p><img src="https://www.yuthon.com/images/WGAN_figure_7.jpg" alt="WGAN_figure_7" /></p>

<h3 id="wgan-gp">WGAN-GP</h3>

<p>上面提到，WGAN让$f_w$满足K-Lipschitz条件的方法很粗暴，直接将整个网络的权重都限制在了一个很小的区间内。这样做其实是会导致一些问题的。</p>

<ul>
<li>第一个问题是<strong>weight clipping不能充分利用神经网络的容量（capacity underuse）</strong>。如下图所示，左图第一排是WGAN估计的分布（value surface），第二排是WGAN-GP估计的分布。可以明显看出，WGAN基本上都是用矩形来近似，没有像WGAN-GP那样学到数据样本更高层的变化趋势。从右图的第二幅图可以看出，经过充分训练的WGAN网络的权值基本上就分布在了$-c$与$c$两个端点上，中间几乎没有权值分布。这简直就像二值神经网络一样了，难怪网络的容量不能得到充分利用。</li>
<li>第二个问题是<strong>梯度消失与梯度爆炸（exploding and vanishing gradients）</strong>。从右图的第一幅图可以看到，在不用BatchNorm的时候，weight clipping的常数$c$稍有不同，就会使得梯度随着网络一层层传播而指数增长或者衰减，从而导致梯度消失或者爆炸。</li>
</ul>

<p><img src="https://www.yuthon.com/images/WGAN-GP_figure_1.jpg" alt="WGAN-GP_figure_1" /></p>

<p>针对这两个问题，WGAN的作者又想出了新的操作——gradient decay。新的操作能够解决上述的两个问题，让WGAN的训练过程更加稳定（好像上次WGAN对DCGAN的时候也是这么讲的:cry:）。</p>

<p>上面说了，weight clipping是为了让网络函数$D(x)$满足Lipschitz条件才用的，并不是非它不可。既然表现这么差，那我们能不能换一种方法来做Lipschitz约束呢？gradient decay就是这样被想出来的。我们回顾一下Lipschitz条件的定义，是要求函数$D(x)$在它的值域上满足$\forall x_1, x_2, \parallel \nabla D(x) \parallel _p \le K$。于是作者们<strong>在判别器的损失函数上加了一个梯度的惩罚项，鼓励梯度的范数趋近于K</strong>（或者说是1，反正1-Lipschitz与K-Lipschitz条件只是差了常数倍）
$$
L = \underset{\tilde{x} \sim \mathbb{P}_g}{\mathbb{E}} [D(\tilde{x})] - \underset{x \sim \mathbb{P}_r}{\mathbb{E}} [D(x)] + \lambda \underset{\hat{x} \sim \mathbb{P}_{\hat{x}}}{\mathbb{E}} [(\parallel \nabla _{\hat{x}} D(\hat{x})\parallel _2 -1)^2]
$$
这里有几个需要注意的地方：</p>

<ul>
<li><p><strong>最优判别器的一个特性（properties of the optimal WGAN critic）</strong>：为什么惩罚项是鼓励梯度绝对值趋向于$K$，而不是只是让梯度在$[-K, K]$之间呢？这就牵扯到WGAN训练出来的最优判别器的一个性质。假设我们随机采到了两个样本$x_r \sim \mathbb{P} _ r$和$x_g \sim \mathbb{P} _ g$，那么对所有在其连线$x_t = (1-t)x_g + t x_r$上的点的梯度就是$\nabla D(x_t) = \frac{x_r - x_t}{\parallel y - x_t \parallel}$。换句话说，在$\mathbb{P}_r$与$\mathbb{P}_g$内，以及两者的中间地带，最优判别器的梯度基本上都是1（或者说$K$）。</p></li>

<li><p><strong>$\mathbb{P}_{\hat{x}}$的采样分布（sampling distribution）</strong>：在计算$\lambda \underset{\hat{x} \sim \mathbb{P}_{\hat{x}}}{\mathbb{E}} [(\parallel \nabla _{\hat{x}} D(\hat{x})\parallel _2 -1)^2]$的时候，需要把期望$\mathbb{E}$转换为采样然后取平均。然而这个$\mathbb{P}_{\hat{x}}$要求我们在整个样本空间$\mathcal{X}$上采样，这种在高维空间指望用采样来估计期望的方法显然是不可实现的，会导致维度灾难。于是文中将$\mathbb{P}_{\hat{x}}$的采样定义为在沿着真实分布$\mathbb{P}_r$的一堆点与生成分布$\mathbb{P}_g$的一堆点的直线上均匀采样。从实验效果上来说，这是很有效的。</p></li>

<li><p><strong>惩罚系数（penalty coefficient）</strong>：取$\lambda=10$，作者称无论在Toy数据集还是大规模的ImageNet数据及上都是work的。</p></li>

<li><p><strong>不要用Batch Normalization</strong>：gradient penalty惩罚的是单个对单个的样本，而BN会把这样单个的映射变成整个batch的所有输入到整个batch的输出，引入了不同采样之间的依赖关系，这样GP就不能用了。作者推荐用Layer Normalization。</p></li>

<li><p><strong>双边惩罚（two-sided penalty）</strong>：惩罚项鼓励梯度从正负两边来趋向1（或者说$K$），作者称这样能收敛到更好的最优点，速度更快，而且对于判别器来说不会造成太大的约束。</p></li>
</ul>

<p>WGAN-GP相对于WGAN的改进就是上面这些了，其算法的伪代码如下。</p>

<p><img src="https://www.yuthon.com/images/WGAN-GP_algorithm.jpg" alt="WGAN-GP_algorithm" /></p>

<p>接下来看看实验结果：</p>

<ul>
<li>WGAN-GP的收敛速度明显比WGAN快，虽然在wall time上还是比不过DCGAN，但是WGAN-GP在训练的稳定性上犹有过之。</li>
</ul>

<p><img src="https://www.yuthon.com/images/WGAN-GP_figure_3.jpg" alt="WGAN-GP_figure_3" /></p>

<ul>
<li>同时，在面对之前的一些困难的训练条件（比如说不用BN层甚至不用任何normalization），WGAN-GP都能够稳定训练并且结果良好。特别是面对ResNet-101这么高层的网络，只有WGAN-GP能训练出结果。</li>
</ul>

<p><img src="https://www.yuthon.com/images/WGAN-GP_figure_2.jpg" alt="WGAN-GP_figure_2" /></p>

<h2 id="gans-in-practice">GANs in Practice</h2>

<h3 id="mlp-gan">MLP-GAN</h3>

<p>使用多层感知机（MLP）来构建判别器与生成器的网络，可以说得上是最为简单的GANs了。可以根据这个来摸清楚GANs自身的一套工作流程到底是怎么样的，为之后实现复杂的GANs网络做个铺垫。本小节相关代码在<a href="https://github.com/corenel/GAN-Zoo/tree/master/GAN">GAN-Zoo/GAN</a>中。</p>

<p>首先我们需要训练集，比如说我们这里用到的MNIST数据集。PyTorch在这点上做得很好，对于一些常用的数据集都自带有loader，不用自己写了。相关代码见<a href="https://github.com/corenel/GAN-Zoo/blob/master/GAN/data_loader.py">GAN-Zoo/GAN/data_loader.py</a>。</p>

<p>有了数据集之后就需要自己定义模型的网络结构了，具体到GANs就是<a href="https://github.com/corenel/GAN-Zoo/blob/master/GAN/models.py#L6-L22">判别器</a>与<a href="https://github.com/corenel/GAN-Zoo/blob/master/GAN/models.py#L25-L41">生成器</a>的定义。这里贴一段判别器的定义，可以看出PyTorch在网络定义方面还是很方便的（和Keras差不多）。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">Discriminator</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;Model for Discriminator.&#34;&#34;&#34;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
        <span class="s2">&#34;&#34;&#34;Init for Discriminator model.&#34;&#34;&#34;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Discriminator</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">),</span>
                                   <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
                                   <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">),</span>
                                   <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
                                   <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">),</span>
                                   <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="s2">&#34;&#34;&#34;Forward step for Discriminator model.&#34;&#34;&#34;</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span></code></pre></td></tr></table>
</div>
</div>
<p>定义完模型，之后就是整个网络的训练过程了：</p>

<ul>
<li><a href="https://github.com/corenel/GAN-Zoo/blob/master/GAN/main.py#L17-L37">初始化阶段</a>：初始化models、criterion（<code>nn.BCELoss()</code>）以及optimizer（<code>nn.optim.Adam()</code>），检查cuda是否可用（<code>nn.cuda.is_available()</code>），能用的话就上GPU跑。</li>
<li>网络训练阶段：

<ul>
<li><a href="https://github.com/corenel/GAN-Zoo/blob/master/GAN/main.py#L49-L78">训练判别器</a>：主要分为两个步骤，首先从数据集中读取样本，判别器forward一遍，然后和真实标签（<code>1</code>）做loss并backward；其次，生成随机噪声而后经过生成器的forward得到生成样本，再喂给判别器，与虚假标签（<code>0</code>）做loss并backward。最后由optimizer更新判别器网络的参数。</li>
<li><a href="https://github.com/corenel/GAN-Zoo/blob/master/GAN/main.py#L80-L105">训练生成器</a>：首先生成随机噪声，而后通过生成器网络生成虚假样本，再通过判别器网络得到loss，并更新生成器网络。值得注意的是，<a href="https://github.com/corenel/GAN-Zoo/blob/master/GAN/main.py#L91">生成器的loss的计算</a>用的是真实标签（<code>1</code>），也就是上述的heuristicly designed non-saturating cost。</li>
<li><a href="https://github.com/corenel/GAN-Zoo/blob/master/GAN/main.py#L107-L131">输出log并保存model</a></li>
</ul></li>
</ul>

<p>非常简单的代码，但是生成出来的数字的效果还是很不错的。</p>

<p><img src="https://www.yuthon.com/images/GAN_real_images.jpg" alt="GAN_real_images" /></p>

<p><img src="https://www.yuthon.com/images/GAN_fake_images-300.jpg" alt="GAN_fake_images-300" /></p>

<p>第一张是MNIST数据集中的，第二张是通过GANs生成的（300次迭代）。虽然第二张还有些不尽如人意之处（迭代次数太少），但是总体上来说，已经非常接近真实的数字图片了。这就是GANs的威力！</p>

<h3 id="dcgan-1">DCGAN</h3>

<p>DCGAN与MLP-GAN的代码相差不多，基本上就是重新写一遍model的事。为了测试DCGAN的capacity，我将MNIST数据集换成了CIFAR-10数据集。相关代码见<a href="https://github.com/corenel/GAN-Zoo/tree/master/DCGAN">GAN-Zoo/DCGAN</a>。</p>

<p>不过这次的结果只能说是差强人意，20次迭代后生成的图像还算不错（毕竟CIFAR-10的分辨率是<code>28*28</code>）：</p>

<p><img src="https://www.yuthon.com/images/DCGAN-fake-20-700.jpg" alt="DCGAN-fake-20-700" /></p>

<p>但是继续训练的话，GANs训练不稳定的问题就出现了。到24次迭代的时候，由于判别器已经非常精准，导致生成器的loss固定在了27左右动弹不得，从而生成的图像变成了一团噪声：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">Epoch <span class="o">[</span><span class="m">24</span>/25<span class="o">]</span> Step <span class="o">[</span><span class="m">200</span>/782<span class="o">]</span>:d_loss<span class="o">=</span><span class="m">2</span>.160674767992532e-07 <span class="nv">g_loss</span><span class="o">=</span><span class="m">27</span>.614051818847656 D<span class="o">(</span>x<span class="o">)=</span><span class="m">2</span>.160674767992532e-07 D<span class="o">(</span>G<span class="o">(</span>z<span class="o">))=</span><span class="m">0</span>.0
Epoch <span class="o">[</span><span class="m">24</span>/25<span class="o">]</span> Step <span class="o">[</span><span class="m">210</span>/782<span class="o">]</span>:d_loss<span class="o">=</span><span class="m">6</span>.410500191122992e-06 <span class="nv">g_loss</span><span class="o">=</span><span class="m">27</span>.623014450073242 D<span class="o">(</span>x<span class="o">)=</span><span class="m">6</span>.410500191122992e-06 D<span class="o">(</span>G<span class="o">(</span>z<span class="o">))=</span><span class="m">0</span>.0
Epoch <span class="o">[</span><span class="m">24</span>/25<span class="o">]</span> Step <span class="o">[</span><span class="m">220</span>/782<span class="o">]</span>:d_loss<span class="o">=</span><span class="m">1</span>.5441528375959024e-06 <span class="nv">g_loss</span><span class="o">=</span><span class="m">27</span>.62175750732422 D<span class="o">(</span>x<span class="o">)=</span><span class="m">1</span>.5441528375959024e-06 D<span class="o">(</span>G<span class="o">(</span>z<span class="o">))=</span><span class="m">0</span>.0
Epoch <span class="o">[</span><span class="m">24</span>/25<span class="o">]</span> Step <span class="o">[</span><span class="m">230</span>/782<span class="o">]</span>:d_loss<span class="o">=</span><span class="m">3</span>.24100881243794e-07 <span class="nv">g_loss</span><span class="o">=</span><span class="m">27</span>.62472152709961 D<span class="o">(</span>x<span class="o">)=</span><span class="m">3</span>.24100881243794e-07 D<span class="o">(</span>G<span class="o">(</span>z<span class="o">))=</span><span class="m">0</span>.0
...</code></pre></td></tr></table>
</div>
</div>
<p><img src="https://www.yuthon.com/images/DCGAN-fake-24-300.jpg" alt="DCGAN-fake-24-300" /></p>

<p>不过到了第25次迭代，DCGAN似乎又略微恢复了正常：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-Shell" data-lang="Shell"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-Shell" data-lang="Shell">Epoch <span class="o">[</span><span class="m">25</span>/25<span class="o">]</span> Step <span class="o">[</span><span class="m">10</span>/782<span class="o">]</span>:d_loss<span class="o">=</span><span class="m">0</span>.32297325134277344 <span class="nv">g_loss</span><span class="o">=</span><span class="m">8</span>.964262962341309 D<span class="o">(</span>x<span class="o">)=</span><span class="m">0</span>.3229268193244934 D<span class="o">(</span>G<span class="o">(</span>z<span class="o">))=</span><span class="m">4</span>.6418874262599275e-05
Epoch <span class="o">[</span><span class="m">25</span>/25<span class="o">]</span> Step <span class="o">[</span><span class="m">20</span>/782<span class="o">]</span>:d_loss<span class="o">=</span><span class="m">0</span>.006471103988587856 <span class="nv">g_loss</span><span class="o">=</span><span class="m">7</span>.038626194000244 D<span class="o">(</span>x<span class="o">)=</span><span class="m">0</span>.0035153746139258146 D<span class="o">(</span>G<span class="o">(</span>z<span class="o">))=</span><span class="m">0</span>.002955729141831398
Epoch <span class="o">[</span><span class="m">25</span>/25<span class="o">]</span> Step <span class="o">[</span><span class="m">30</span>/782<span class="o">]</span>:d_loss<span class="o">=</span><span class="m">0</span>.17143061757087708 <span class="nv">g_loss</span><span class="o">=</span><span class="m">12</span>.035135269165039 D<span class="o">(</span>x<span class="o">)=</span><span class="m">0</span>.17115993797779083 D<span class="o">(</span>G<span class="o">(</span>z<span class="o">))=</span><span class="m">0</span>.0002706760715227574
Epoch <span class="o">[</span><span class="m">25</span>/25<span class="o">]</span> Step <span class="o">[</span><span class="m">40</span>/782<span class="o">]</span>:d_loss<span class="o">=</span><span class="m">0</span>.21678031980991364 <span class="nv">g_loss</span><span class="o">=</span><span class="m">11</span>.419050216674805 D<span class="o">(</span>x<span class="o">)=</span><span class="m">0</span>.004731819964945316 D<span class="o">(</span>G<span class="o">(</span>z<span class="o">))=</span><span class="m">0</span>.2120485007762909</code></pre></td></tr></table>
</div>
</div>
<p><a href="https://www.yuthon.com/images/DCGAN-fake-25-700.jpg">DCGAN-fake-25-700</a></p>

<p>当然，让GANs训练变得稳定的方法不是没有，<a href="https://github.com/soumith/ganhacks">这里</a>就列举了不少tricks：</p>

<ul>
<li>避免稀疏的梯度：不要使用ReLU或者Max Pooling，尽量用LeakyReLU；</li>
<li>使用软标签（soft and noisy labels），也就是说真实标签与虚假标签不要是固定的<code>1</code>或者<code>0</code>，最好加点噪声上去；</li>
<li>在真实样本的输入上也加点随时间衰减的噪声</li>
<li>给生成器加Dropout层</li>
<li>……</li>
</ul>

<p>这样的trick是还有很多，有些我试过确实有用，还有些则不是很确定，属于玄学范畴。不过与其用一大堆tricks，还不如直接简单粗暴地上WGAN吧！</p>

<h3 id="wgan">WGAN</h3>

<p>相关代码见<a href="https://github.com/corenel/GAN-Zoo/tree/master/WGAN">GAN-Zoo/WGAN</a>。WGAN在实现上主要有以下几点需要注意：</p>

<ul>
<li>判别器模型的输出不过Sigmoid层，而是取平均之后flatten到1维，输出的是Wasserstein距离而非分类结果。（<a href="https://github.com/corenel/GAN-Zoo/blob/master/WGAN/models.py#L81-L82">code</a>）</li>
<li>由于判别器器在一个epoch中需要训练多次，因此不能用<code>for..in..</code>来loop整个书记，而是用<code>iterator</code>来迭代循环。（<a href="https://github.com/corenel/GAN-Zoo/blob/master/WGAN/main.py#L68-L69">code</a>）</li>
<li>在生成器训练过25次之前，或者是之后每500次的时候，判别器在每个epoch内都需要训练100次而非默认的5次。这是为了让判别器在一开始就达到差不多最优的状态。（<a href="https://github.com/corenel/GAN-Zoo/blob/master/WGAN/main.py#L58-L64">code</a>）</li>
<li>为了提升效率，优化生成器的时候用判别器得到loss不更新判别器的梯度（<a href="https://github.com/corenel/GAN-Zoo/blob/master/WGAN/main.py#L95-L97">code</a>），优化判别器用生成器生成虚假样本时也不更新生成器的梯度（<a href="https://github.com/corenel/GAN-Zoo/blob/master/WGAN/main.py#L78-L83">code</a>）。</li>
<li>在生成器的optimizer更新权值之后clamp所有的权值，使其在一定范围内，以满足K-Lipschitz条件。（<a href="https://github.com/corenel/GAN-Zoo/blob/master/WGAN/main.py#L88-L90">code</a>）</li>
</ul>

<h3 id="wgan-gp-1">WGAN-GP</h3>

<p>相关代码见<a href="https://github.com/corenel/GAN-Zoo/tree/master/WGAN-GP">GAN-Zoo/WGAN-GP</a>。WGAN-GP在实现上与WGAN的主要区别在于：</p>

<ul>
<li>用gradient penalty代替了weight clipping。这里面涉及到计算梯度的梯度，这在PyTorch 0.2.0版本前是无法做到的，我猜大概也是因为这个原因，原作者选择了用TensorFlow来实现WGAN-GP。关于在低版本的PyTorch上实现gradient penalty的讨论<a href="https://discuss.pytorch.org/t/how-to-implement-gradient-penalty-in-pytorch/1656">见此</a>。PyTorch版本更新之后增加了<code>torch.autograd.grad</code>这么一个函数，能够满足我们的需求。</li>
<li>在判别器中不使用Batch Normalization，详细原因在前文已经给出。可以选择的替代有Layer Normaliztion，Weight Normalization等。</li>
</ul>

<p>在代码与实验中，有以下几点需要注意：</p>

<ul>
<li>在训练的初始阶段，由于增大了判别器的训练次数（1个epoch训练100次），判别器接近最优，导致刚开始的loss非常大（~ -150000）。在判别器训练次数恢复正常后（1个epoch训练5次），loss的绝对值逐渐下降到几千几百的级别。迭代10000次左右即收敛到比较好的效果。</li>
<li>我的代码实现里，对于判别器模型没有使用任何Normalization，加上的话收敛速度应该会有一定提升。</li>
</ul>

<h2 id="further-reading">Further Reading</h2>

<p>最后总结一些我在学习过程中看过的比较好的资料以及代码实现：</p>

<ul>
<li><a href="https://github.com/yunjey/pytorch-tutorial">pytorch-tutorial</a>: 我见到的最好的PyTorch入门教程，简洁清晰明了，有其他DL框架使用经验以及Python基础的朋友适用。一上来看不懂的话，可以先看官方的60分钟入门教程之后，再看这个。</li>
<li><a href="http://arxiv.org/abs/1701.00160">NIPS 2016 Tutorial: Generative Adversarial Networks</a>: Iran Goodfellow在NIPS2016上的教程演讲，很好地介绍了GANs的基本思想和应用。前面的数学推导看不懂的话，可以结合<a href="http://www.deeplearningbook.org/">DeepLearningBook</a>（<a href="http://www.epubit.com.cn/book/details/4278">中译版</a>已经上市，本人忝为校对之一）。同时，还可以结合slides看，slides上的都写得很简练，tutorial中则做了详细的说明。可惜的是，当时WGAN及其变种还没有出来，因此在这篇tutorial中没有提到。</li>
<li>几篇代表论文以及相关的代码实现：

<ul>
<li>GAN: <a href="https://arxiv.org/abs/1406.2661">paper</a>, <a href="https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/generative_adversarial_network/main.py#L34-L50">code (pytorch-tutorial)</a></li>
<li>DCGAN: <a href="https://arxiv.org/abs/1511.06434">paper</a>, <a href="https://github.com/pytorch/examples/tree/master/dcgan">code (PyTorch official example)</a>, <a href="https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/03-advanced/deep_convolutional_gan">code (pytorch-tutorial)</a></li>
<li>WGAN: <a href="https://arxiv.org/abs/1701.07875">paper</a>, <a href="https://github.com/martinarjovsky/WassersteinGAN">code (PyTorch)</a></li>
<li>WGAN-GP: <a href="https://arxiv.org/abs/1704.00028">https://arxiv.org/abs/1704.00028</a>, <a href="https://github.com/caogang/wgan-gp">code (PyTorch)</a></li>
</ul></li>
<li>我自己对上述论文的代码实现，欢迎指正：<a href="https://github.com/corenel/GAN-Zoo">GAN-Zoo</a></li>
<li>一些有趣的GANs应用

<ul>
<li><a href="http://make.girls.moe/technical_report.pdf">Create Anime Characters with A.I. !</a>：一篇非常有意思的技术文章，生成的头像插图质量非常高。（<a href="http://make.girls.moe/">online demo</a>）</li>
</ul></li>
</ul>
    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">Author</span>
    <span class="item-content">Yusu Pan</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">LastMod</span>
    <span class="item-content">
        2017-08-12
        
    </span>
  </p>
  
  <p class="copyright-item">
    <span class="item-title">License</span>
    <span class="item-content"><a rel="license noopener" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank">CC BY-NC-ND 4.0</a></span>
  </p>
</div>
<footer class="post-footer">
      <div class="post-tags">
          <a href="https://www.yuthon.com/tags/generative-adversarial-networks/">Generative Adversarial Networks</a>
          <a href="https://www.yuthon.com/tags/gans/">GANs</a>
          <a href="https://www.yuthon.com/tags/dcgan/">DCGAN</a>
          <a href="https://www.yuthon.com/tags/wgan/">WGAN</a>
          <a href="https://www.yuthon.com/tags/wgan-gp/">WGAN-GP</a>
          <a href="https://www.yuthon.com/tags/deep-learning/">Deep Learning</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="https://www.yuthon.com/post/papers/notes-for-adversarial-discriminative-domain-adaptation/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">Notes for Adversarial Discriminative Domain Adaptation</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        <a class="next" href="https://www.yuthon.com/post/papers/notes-for-amortized-inference-and-learning-in-latent-crf/">
            <span class="next-text nav-default">Notes for Amortized Inference and Learning in Latent CRF</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        <div id="disqus_thread"></div>
    <script type="text/javascript">
    (function() {
      
      
      if (window.location.hostname === 'localhost') return;

      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      var disqus_shortname = 'yuthons-blog';
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:xxdsox@gmail.com" class="iconfont icon-email" title="email"></a>
      <a href="https://stackoverflow.com/users/5682817" class="iconfont icon-stack-overflow" title="stack-overflow"></a>
      <a href="https://twitter.com/corenel" class="iconfont icon-twitter" title="twitter"></a>
      <a href="https://www.facebook.com/xxdsox" class="iconfont icon-facebook" title="facebook"></a>
      <a href="https://github.com/corenel" class="iconfont icon-github" title="github"></a>
      <a href="https://www.zhihu.com/people/pan-yu-su" class="iconfont icon-zhihu" title="zhihu"></a>
  <a href="https://www.yuthon.com/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2017 - 
    2019
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">© This post is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License, please give source if you wish to quote or reproduce.
</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>
<script type="text/javascript" src="https://www.yuthon.com/dist/even.26188efa.min.js"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      TeX: {equationNumbers: {autoNumber: "AMS"}},
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script type="text/javascript" async src="https://www.yuthon.com/lib/mathjax/math-code.js"></script>
    <script async src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"  integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script>


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-76233259-1', 'auto');
	ga('set', 'anonymizeIp', true);
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>







</body>
</html>
