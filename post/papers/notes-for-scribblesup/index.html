<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>Notes for ScribbleSup - Yuthon&#39;s Blog</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="Yusu Pan" /><meta name="description" content="毕设需要写一个图像标注的软件, 来给场景分割的数据集做标注. 经学长推荐, 看了今年的这篇文章, 作者中竟然还有 Kaiming He 大神, 给微软膜一秒.
这篇文章讲了一个弱监督的场景分割的算法 ScribbleSup, 主要是先通过 Graph Cut 将输入的 scribble 信息广播到没有标注的像素, 然后用 FCN 来做像素级别的预测. 令人遗憾的是 Github 上并没有人实现 (不能偷懒了TAT).
" /><meta name="keywords" content="yuthon, yusu-pan, blog, deep-learning, computer-vision" />






<meta name="generator" content="Hugo 0.54.0 with even 4.0.0" />


<link rel="canonical" href="https://www.yuthon.com/post/papers/notes-for-scribblesup/" />
<link rel="apple-touch-icon" sizes="180x180" href="https://www.yuthon.com/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://www.yuthon.com/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://www.yuthon.com/favicon-16x16.png">
<link rel="manifest" href="https://www.yuthon.com/manifest.json">
<link rel="mask-icon" href="https://www.yuthon.com/safari-pinned-tab.svg" color="#5bbad5">


<link href="https://www.yuthon.com/dist/even.c2a46f00.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="Notes for ScribbleSup" />
<meta property="og:description" content="毕设需要写一个图像标注的软件, 来给场景分割的数据集做标注. 经学长推荐, 看了今年的这篇文章, 作者中竟然还有 Kaiming He 大神, 给微软膜一秒.

这篇文章讲了一个弱监督的场景分割的算法 ScribbleSup, 主要是先通过 Graph Cut 将输入的 scribble 信息广播到没有标注的像素, 然后用 FCN 来做像素级别的预测. 令人遗憾的是 Github 上并没有人实现 (不能偷懒了TAT)." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.yuthon.com/post/papers/notes-for-scribblesup/" />
<meta property="article:published_time" content="2016-11-20T18:26:24&#43;00:00"/>
<meta property="article:modified_time" content="2016-11-20T18:26:24&#43;00:00"/>

<meta itemprop="name" content="Notes for ScribbleSup">
<meta itemprop="description" content="毕设需要写一个图像标注的软件, 来给场景分割的数据集做标注. 经学长推荐, 看了今年的这篇文章, 作者中竟然还有 Kaiming He 大神, 给微软膜一秒.

这篇文章讲了一个弱监督的场景分割的算法 ScribbleSup, 主要是先通过 Graph Cut 将输入的 scribble 信息广播到没有标注的像素, 然后用 FCN 来做像素级别的预测. 令人遗憾的是 Github 上并没有人实现 (不能偷懒了TAT).">


<meta itemprop="datePublished" content="2016-11-20T18:26:24&#43;00:00" />
<meta itemprop="dateModified" content="2016-11-20T18:26:24&#43;00:00" />
<meta itemprop="wordCount" content="3054">



<meta itemprop="keywords" content="ScribbleSup,Deep Learning,Scene Segmentation," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Notes for ScribbleSup"/>
<meta name="twitter:description" content="毕设需要写一个图像标注的软件, 来给场景分割的数据集做标注. 经学长推荐, 看了今年的这篇文章, 作者中竟然还有 Kaiming He 大神, 给微软膜一秒.

这篇文章讲了一个弱监督的场景分割的算法 ScribbleSup, 主要是先通过 Graph Cut 将输入的 scribble 信息广播到没有标注的像素, 然后用 FCN 来做像素级别的预测. 令人遗憾的是 Github 上并没有人实现 (不能偷懒了TAT)."/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="https://www.yuthon.com/" class="logo">Yuthon&#39;s Blog</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="https://www.yuthon.com/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="https://www.yuthon.com/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a><a href="https://www.yuthon.com/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="https://www.yuthon.com/about/">
        <li class="mobile-menu-item">About</li>
      </a><a href="https://www.yuthon.com/resume/">
        <li class="mobile-menu-item">Resume</li>
      </a><a href="https://github.com/corenel">
        <li class="mobile-menu-item">Works</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="https://www.yuthon.com/" class="logo">Yuthon&#39;s Blog</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="https://www.yuthon.com/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="https://www.yuthon.com/categories/">Categories</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="https://www.yuthon.com/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="https://www.yuthon.com/about/">About</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="https://www.yuthon.com/resume/">Resume</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="https://github.com/corenel">Works</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">Notes for ScribbleSup</h1>

      <div class="post-meta">
        <span class="post-time"> 2016-11-20 </span>
        <div class="post-category">
            <a href="https://www.yuthon.com/categories/notes/"> Notes </a>
            </div>
          <span class="more-meta"> 3054 words </span>
          <span class="more-meta"> 7 mins read </span>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  <div class="post-toc-content always-active">
    <nav id="TableOfContents">
<ul>
<li>
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#scribble-supervised-learning">Scribble-Supervised Learning</a>
<ul>
<li><a href="#objective-functions">Objective Functions</a></li>
<li><a href="#optimization">Optimization</a></li>
<li><a href="#related-work">Related Work</a>
<ul>
<li><a href="#graphical-models-for-segmentation">Graphical models for segmentation</a></li>
<li><a href="#weakly-supervised-semantic-segmentation">Weakly-supervised semantic segmentation</a></li>
</ul></li>
</ul></li>
<li><a href="#experiment">Experiment</a>
<ul>
<li><a href="#annotating-scribbles">Annotating Scribbles</a></li>
<li><a href="#experiments-on-pascal-voc-2012">Experiments on PASCAL VOC 2012</a>
<ul>
<li><a href="#strategies-of-utilizing-scribbles">Strategies of utilizing scribbles</a></li>
<li><a href="#sensitivities-to-scribble-quality">Sensitivities to scribble quality</a></li>
<li><a href="#comparisons-with-other-weakly-supervised-methods">Comparisons with other weakly-supervised methods</a></li>
<li><a href="#comparisons-with-using-masks">Comparisons with using masks</a></li>
</ul></li>
<li><a href="#experiments-on-pascal-context">Experiments on PASCAL-CONTEXT</a></li>
</ul></li>
</ul></li>
</ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <p>毕设需要写一个图像标注的软件, 来给场景分割的数据集做标注. 经学长推荐, 看了今年的这篇文章, 作者中竟然还有 Kaiming He 大神, 给微软膜一秒.</p>

<p>这篇文章讲了一个弱监督的场景分割的算法 ScribbleSup, 主要是先通过 Graph Cut 将输入的 scribble 信息广播到没有标注的像素, 然后用 FCN 来做像素级别的预测. 令人遗憾的是 Github 上并没有人实现 (不能偷懒了TAT).</p>

<h2 id="introduction">Introduction</h2>

<p>TBD</p>

<h2 id="scribble-supervised-learning">Scribble-Supervised Learning</h2>

<h3 id="objective-functions">Objective Functions</h3>

<p>主要用到的记号如下:</p>

<table>
<thead>
<tr>
<th>Symbol</th>
<th>Name</th>
<th>Note</th>
</tr>
</thead>

<tbody>
<tr>
<td>$X$</td>
<td>training image</td>
<td></td>
</tr>

<tr>
<td>${x_i}$</td>
<td>set of non-overlapping superpixles</td>
<td>$\cup_i x_i = X; x_i \cap x_j = \varnothing, \forall i,j$</td>
</tr>

<tr>
<td>$S$</td>
<td>scribble annotations of image</td>
<td>$S={s_k, c_k}$</td>
</tr>

<tr>
<td>$s_k$</td>
<td>the pixels of a scribble $k$</td>
<td></td>
</tr>

<tr>
<td>$c_k$</td>
<td>the scribble’s category label</td>
<td>$0 \le c_k \le C$; $c_k=0$ for background</td>
</tr>

<tr>
<td>$Y$ or ${y_i}$</td>
<td>the category label of ${x_i}$</td>
<td>provides full annotations of the image</td>
</tr>
</tbody>
</table>

<p>定义目标函数为</p>

<p>$$\sum_i \psi_i (y_i | X,S) + \sum_{i,j} \psi_{ij} (y_i, y_j | X)$$</p>

<p>其中$\psi_i$是一个关于$x_i$的一元项 (unary term), 而$\psi\ _{ij}$是关于$x_i$与$x_j$的成对项 (pairwise term).</p>

<ul>
<li><p>$\psi _i$由两个部分组成, 一个是$\psi ^{scr}_i$, 另一个是$\psi^{net}_i$.两者权重相同, $\psi _i = \psi^{scr} _i + \psi^{net} _i$.</p>

<ul>
<li><p>$\psi ^{scr}_i$ 基于 scribble, 定义如下:
$$
\psi ^{scr}_i=
\begin{aligned}
&amp;0 &amp; \text{if $y_i=c_k$ and $x_i \cap s_k \ne \varnothing$}\<br />
&amp;-log(\frac{1}{|c_k|}) &amp; \text{if $y_i \in {c_k}$ and $x_i \cap S = \varnothing$} \<br />
&amp;\infty &amp; \text{otherwise} \<br />
\end{aligned}
$$</p></li>

<li><p>当$x_i$与$s_k$有交集, 且标签是分到$c_k$时, 则$cost=0$</p></li>

<li><p>当$x_i$与所有 scribble 都没有交集, 则它可以被等概率地分给任何标签. 当然, $y_i$需要在${c_k}$之内. 此处$|{c_k}|$表示标签集内元素个数.</p></li>

<li><p>如果不是以上两种情况, 则$cost= \infty$</p></li>

<li><p>$\psi ^{net}_i$基于 FCN 的输出, 定义为
$$
\psi^{net}_i (y_i) = -log P(y_i|X, \Theta)
$$</p></li>

<li><p>$\Theta$表示网络的参数</p></li>

<li><p>$log P(y_i|X, \Theta)$表示了$x_i$属于标签$y_i$的对数概率, 实际上是$x_i$内所有像素的对数概率之和.</p></li>
</ul></li>

<li><p>$\psi_{ij}$用以衡量相邻的两个超像素的相似程度, 主要是用色彩直方图与纹理直方图来量化 (均已归一化).
$$
\psi_{ij} (y_i, y_j | X) = [y_i \ne y_j] exp \left( -\frac{||h_c(x_i) - h_c(x_j)||^2_2}{\delta_c^2} - \frac{||h_t(x_i) - h_t(x_j)||^2_2}{\delta_t^2} \right)
$$</p>

<ul>
<li>$h_c(x_i)$ 表示RGB三个 channel 每个 channel 分成 25 bins 的色彩直方图</li>
<li>$h_t(x_i)$ 表示横向与纵向的梯度直方图, 每个方向 10 bins</li>
<li>$[\cdot]$表示一个符号函数, 条件为真则为$1$, 否则为$0$</li>
<li>$\delta_c=5, \delta_t = 10$</li>
<li>对于不是同一个标签的临近超像素来说, 它们间的外观越相似, 则 cost 越大</li>
</ul></li>
</ul>

<p>最后把上边这些合起来, 就成了一个对于以下式子进行最优化的问题:
$$
\sum_i \psi^{scr}_i (y_i |X, S) + \sum_i -log P(y_i  | X, \Theta) + \sum_{i,j} \psi_{ij} (y_i, y_j | X)
$$
其中有两组变量, 一个是所有超像素的标签$Y={y_i}$, 另一个是 FCN 的参数 $\Theta$.</p>

<p><img src="https://www.yuthon.com/images/ScribbleSup_grapgical_model.png" alt="ScribbleSup_grapgical_model" /></p>

<h3 id="optimization">Optimization</h3>

<p>论文里采用的是一种交替优化的方法:</p>

<ul>
<li>$\Theta$固定, 优化$Y$, slover 基于 scribbles, appearance 以及 FCN 网络的预测, 将标签传播到未标记的像素中</li>
<li>$Y$固定, 优化$\Theta$, slover 对 pixel-wise 的语义分割的 FCN 进行学习</li>
</ul>

<p>具体的来说就是</p>

<ul>
<li><strong>Propagating scribble information to unmarked pixels</strong></li>
</ul>

<p>当$\Theta$固定时, 一元项$\psi _i = \psi^{scr} _i + \psi^{net} _i$能够用列举所有可能的标签$0 \le y_i \le C$得到, 成对项也能够预先计算生成一个 look-up table. 因此, 优化问题就能用 graph cut 的方法来解决. 论文里用的是<a href="http://www.csd.uwo.ca/faculty/yuri/Papers/pami04.pdf">这一篇文章</a>的<a href="http://vision.csd.uwo.ca/code/gco- v3.0.zip">现成代码</a>.</p>

<ul>
<li><strong>Optimizing network parameters</strong></li>
</ul>

<p>前一步做完后, 所有超像素的标签都已经定好了, 也就是说$Y$固定了. 之后优化$\Theta$就相当于用$Y$做为监督来训练 FCN. $Y$有了, 那么每个像素的标签就有了, 然后 FCN 面对的就是一个 pixel-wise 的回归问题. FCN 的最后一层输出的就是每个像素的分类的对数概率, 可以用来更新 graph 上的一元项.</p>

<p>训练的时候有几点需要注意:</p>

<ul>
<li>初始化的时候没有 network prediction, 因此就是直接用 graph cut 初始化的. 之后则是在两步之间不断迭代.</li>
<li>每次 network optimizing step 的时候, 前50k次用0.0003的 learning rate, 后10k次用0.0001的 learning rate, batch size 为 8.</li>
<li>每次 network optimizing step 都是从一个 pre-trained 的 model (比如 VGG-16) 重新初始化的. 作者也试过复用上一次迭代后的权重, 但是效果不是很理想. 似乎是由于本来标签就不可靠, 导致训练的时候参数被调到了不太好的局部最优里面.</li>
<li>基本上3次迭代就能得到比较好的效果了, 再多得到的提升微乎其微.</li>
<li>做验证的时候只要用 FCN 就好了, 超像素和 graph model 之类的都只是用来训练用的.</li>
<li>Post-process 用了 CRF.</li>
</ul>

<p>迭代结果如下:</p>

<p><img src="https://www.yuthon.com/images/ScribbleSup_training.png" alt="ScribbleSup_training" /></p>

<h3 id="related-work">Related Work</h3>

<h4 id="graphical-models-for-segmentation">Graphical models for segmentation</h4>

<p>Graphical model 在交互式的图像分割和语义分割领域是很常见的, 通常是目标函数包含了一元项和成对项, 特别适用于对局部和全局的空间约束的建模.</p>

<p>有趣的是, FCN 作为目前最成功的语义分割的方法之一, 由于做的是 pixel-wise 的 regression, 因此其目标函数只有一元项. 不过像 CRF/MRF 这样给 FCN 做 post-processing 或是 joint-training 的方法在之后也发展起来了.</p>

<p>但是这一类 graph model 都是强监督的, 主要工作是在优化 mask 的边缘, 而 ScribbleSup 里面的 graph model 主要是用来把标签传播到其他未标注的像素上. 同时, 这类方法是 pixel-based, 而 ScribbleSup 是 super-pixel-based.</p>

<h4 id="weakly-supervised-semantic-segmentation">Weakly-supervised semantic segmentation</h4>

<p>用 CNN/FCN 来做弱监督的语义分割的方法很多, 用的标注方法也有很多种.</p>

<ul>
<li>Image-level 的标注很容易获取, 但是只用这个的话精度远低于强监督的结果</li>
<li>Box-level 的相比较而言结果与强监督的接近了不少. 由于 Box annotations 本身就提供了物体边缘以及可信的背景区域的信息, 因此就不需要 graph model 来传播标签.</li>
</ul>

<p>这些方法和本篇论文里面讲的 ScribbleSup 比起来到底哪个更胜一筹, 姿势水平更高, 就看下面的实验了.</p>

<h2 id="experiment">Experiment</h2>

<h3 id="annotating-scribbles">Annotating Scribbles</h3>

<p>主要使用了 PASCAL VOC 2012 (20个分类) 以及 PASCAL-CONTEXT (59个分类) 这两个数据集, 同时也标注了 PASCAL VOC 2007 (标注了59个分类). 不过 2007 没有 mask-level 的标注.</p>

<p>总共有10个人在标注, 每张图片一人标注一人检查. 平均下来20分类的话每张图片25秒, 59分类的话每张图片50秒, 算是相当快的了.</p>

<p>同时, 保证每个 object 上的 scribble 至少有其 bounding box 长边的 70% 以上的长度.</p>

<h3 id="experiments-on-pascal-voc-2012">Experiments on PASCAL VOC 2012</h3>

<h4 id="strategies-of-utilizing-scribbles">Strategies of utilizing scribbles</h4>

<p>ScribbleSup 是将标签的扩散与网络的训练合起来考虑的, 但是一个更为简单的方案是把这两步分开来, 先用一些现成的工具 (比如说 GrabCut 或者是 LazySnapping) 把 scribble 转换成 mask, 然后再来训练 FCN 网络. 这个方案听起来也是很吼的, 那么中央到底兹不兹瓷呢, 我们来看看实验结果</p>

<table>
<thead>
<tr>
<th>Method</th>
<th>mIoU(%)</th>
</tr>
</thead>

<tbody>
<tr>
<td>GrabCut + FCN</td>
<td>49.1</td>
</tr>

<tr>
<td>LazySnapping + FCN</td>
<td>53.8</td>
</tr>

<tr>
<td>ours, w/o pairwise terms</td>
<td>60.5</td>
</tr>

<tr>
<td>ours, w/ pairwise terms</td>
<td>63.1</td>
</tr>
</tbody>
</table>

<p>所以说不要听风就是雨, 可以看出分两步走的方案是一个错误的道路, mIoU显著低于 ScribbleSup. 其中的原因主要是这些传统的方法仅仅针对 low-level 的空间或者是色彩信息建模, 并没有考虑到语义的层面. 也就是说, 这些方法得到的 mask 是不值得信赖的, 不能作为 ground truth 来用.</p>

<p>而 ScribbleSup 就不同了, 通过不断的迭代, FCN 能够逐渐学习到 high-level 的语义特征, 这些特征又能反哺给 graph-based scribble propagation. 这样就形成了一个良性循环, 自然 mIoU 就不知比传统方法高到哪里去了.</p>

<p>同时可以看出, 用了成对项的效果比不用的好. 这是因为如果没有了成对项, 那么目标函数就只剩下了一元项, graph cut 步骤变成了基于network prediction 的 winner-take-all 的模式. 这样的话, 信息的传播就只与全卷积有关, 会过于看重局部一致性, 最终导致准确度降低.</p>

<h4 id="sensitivities-to-scribble-quality">Sensitivities to scribble quality</h4>

<p>Scribble quality 是个非常主观的东西, 所以为了研究这个对于准确度的影响, 论文里采用了将原 scribble 放缩为不同长度 (甚至是一个点), 然后实验来观察.</p>

<p><img src="https://www.yuthon.com/images/ScribbleSup_scribble_of_different_length.png" alt="scribble_of_different_length" /></p>

<table>
<thead>
<tr>
<th>Length ratio</th>
<th>mIoU (%)</th>
</tr>
</thead>

<tbody>
<tr>
<td>1</td>
<td>63.1</td>
</tr>

<tr>
<td>0.8</td>
<td>61.8</td>
</tr>

<tr>
<td>0.5</td>
<td>58.5</td>
</tr>

<tr>
<td>0.3</td>
<td>54.3</td>
</tr>

<tr>
<td>0 (spot)</td>
<td>51.6</td>
</tr>
</tbody>
</table>

<p>可以看出, ScribbleSup 对于 scribble length 还是比较鲁棒的, 甚至到了一个点都还能有不错的准确度.</p>

<h4 id="comparisons-with-other-weakly-supervised-methods">Comparisons with other weakly-supervised methods</h4>

<p>All methods are trained on the PASCAL VOC 2012 training images using VGG-16, except that the annotations are different.</p>

<table>
<thead>
<tr>
<th>Method</th>
<th>Annotations</th>
<th>mIoU (%)</th>
</tr>
</thead>

<tbody>
<tr>
<td>MIL-FCN</td>
<td>image-level</td>
<td>25.1</td>
</tr>

<tr>
<td>WSSL</td>
<td>image-level</td>
<td>38.2</td>
</tr>

<tr>
<td>point supervision</td>
<td>spot</td>
<td>46.1</td>
</tr>

<tr>
<td>WSSL</td>
<td>box</td>
<td>60.6</td>
</tr>

<tr>
<td>BoxSup</td>
<td>box</td>
<td>62.0</td>
</tr>

<tr>
<td>ours</td>
<td>spot</td>
<td>51.6</td>
</tr>

<tr>
<td>ours</td>
<td>scribble</td>
<td>63.1</td>
</tr>
</tbody>
</table>

<p>可以看出</p>

<ul>
<li>虽然 image-level 的标注很容易标, 但是训练出来的结果惨不忍睹.</li>
<li>同时, 用 scribble 来标注得到的结果准确度很不错, 并且也是相对比较方便的.</li>
<li>ScribbleSup 即便是用 spot 标注, 结果的 mIoU 也比 point supervision 高了 5%.</li>
</ul>

<h4 id="comparisons-with-using-masks">Comparisons with using masks</h4>

<p>虐了一遍同等级的 weakly-supervised 的方法之后, ScribbleSup 开始对比使用 scribble 和使用 mask 得到的结果. (在 PASCAL VOC 2012 上训练)</p>

<table>
<thead>
<tr>
<th>Supervision</th>
<th># w/ masks</th>
<th># w/scribbles</th>
<th>total</th>
<th>mIoU (%)</th>
</tr>
</thead>

<tbody>
<tr>
<td>weakly</td>
<td>-</td>
<td>11k</td>
<td>11k</td>
<td>63.1</td>
</tr>

<tr>
<td>strongly</td>
<td>11k</td>
<td>-</td>
<td>11k</td>
<td>68.5</td>
</tr>

<tr>
<td>semi</td>
<td>11k</td>
<td>10k (VOC07)</td>
<td>21k</td>
<td>71.3</td>
</tr>
</tbody>
</table>

<p>使用 scribble 比使用 mask 得到的结果差了5%左右, 考虑到这两者标注的困难程度, 这点差距还是可以忍的.</p>

<p>ScribbleSup 其实也是可以用 mask-level 的标注来训练的. 对于 mask-level 的标注, 不使用 graph model, 直接扔到 FCN 的训练里面去就行了. 注意的是这些只能用在 FCN 的训练步骤里, 优化 graph model 这一步骤中不使用. 可以看出, scribble 与 mask 联合起来能达到71.3%的 mIoU, 可以说是非常理想了.</p>

<p><img src="https://www.yuthon.com/images/ScribbleSup_results_on_VOC_2012.png" alt="ScribbleSup_results_on_VOC_2012" /></p>

<h3 id="experiments-on-pascal-context">Experiments on PASCAL-CONTEXT</h3>

<p>To the best of our knowledge, our accuracy is the current state of the art on this dataset. (向dalao低头)</p>

<table>
<thead>
<tr>
<th>Method</th>
<th>Data/Annotations</th>
<th>mIoU (%)</th>
</tr>
</thead>

<tbody>
<tr>
<td>CFM</td>
<td>5k w/ masks</td>
<td>34.4</td>
</tr>

<tr>
<td>FCN</td>
<td>5k w/ masks</td>
<td>35.1</td>
</tr>

<tr>
<td>Boxsup</td>
<td>5k w/ masks + 133k w/ boxes (COCO+VOC7)</td>
<td>40.5</td>
</tr>

<tr>
<td>baseline</td>
<td>5k w/ masks</td>
<td>37.7</td>
</tr>

<tr>
<td>ours, weakly</td>
<td>5k w/ scribbles</td>
<td>36.1</td>
</tr>

<tr>
<td>ours, weakly</td>
<td>5k w/ scribbles + 10k w/ scribbles (VOC07)</td>
<td>39.3</td>
</tr>

<tr>
<td>ours, semi</td>
<td>5k w/ masks + 10k w/ scribbles (VOC07)</td>
<td>42.0</td>
</tr>
</tbody>
</table>

<p><img src="https://www.yuthon.com/images/ScribbleSup_results_on_PASCAL_CONTEXT.png" alt="ScribbleSup_results_on_PASCAL_CONTEXT" /></p>

<p>(To be continued&hellip;)</p>
    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">Author</span>
    <span class="item-content">Yusu Pan</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">LastMod</span>
    <span class="item-content">
        2016-11-20
        
    </span>
  </p>
  
  <p class="copyright-item">
    <span class="item-title">License</span>
    <span class="item-content"><a rel="license noopener" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank">CC BY-NC-ND 4.0</a></span>
  </p>
</div>
<footer class="post-footer">
      <div class="post-tags">
          <a href="https://www.yuthon.com/tags/scribblesup/">ScribbleSup</a>
          <a href="https://www.yuthon.com/tags/deep-learning/">Deep Learning</a>
          <a href="https://www.yuthon.com/tags/scene-segmentation/">Scene Segmentation</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="https://www.yuthon.com/post/practices/train-caffe-yolo-on-our-own-dataset/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">Train Caffe-YOLO on our own dataset</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        <a class="next" href="https://www.yuthon.com/post/papers/notes-for-yolo/">
            <span class="next-text nav-default">Notes for YOLO</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        <div id="disqus_thread"></div>
    <script type="text/javascript">
    (function() {
      
      
      if (window.location.hostname === 'localhost') return;

      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      var disqus_shortname = 'yuthons-blog';
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:xxdsox@gmail.com" class="iconfont icon-email" title="email"></a>
      <a href="https://stackoverflow.com/users/5682817" class="iconfont icon-stack-overflow" title="stack-overflow"></a>
      <a href="https://twitter.com/corenel" class="iconfont icon-twitter" title="twitter"></a>
      <a href="https://www.facebook.com/xxdsox" class="iconfont icon-facebook" title="facebook"></a>
      <a href="https://github.com/corenel" class="iconfont icon-github" title="github"></a>
      <a href="https://www.zhihu.com/people/pan-yu-su" class="iconfont icon-zhihu" title="zhihu"></a>
  <a href="https://www.yuthon.com/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2017 - 
    2019
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">© This post is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License, please give source if you wish to quote or reproduce.
</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>
<script type="text/javascript" src="https://www.yuthon.com/dist/even.26188efa.min.js"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      TeX: {equationNumbers: {autoNumber: "AMS"}},
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"  integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script>


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-76233259-1', 'auto');
	ga('set', 'anonymizeIp', true);
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>







</body>
</html>
